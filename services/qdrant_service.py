#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
qdrant_service.py - Qdrantæ“ä½œã‚µãƒ¼ãƒ“ã‚¹
======================================
Qdrantãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ“ä½œã‚’æ‹…å½“

æ©Ÿèƒ½:
- ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆQdrantHealthCheckerï¼‰
- ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆQdrantDataFetcherï¼‰
- ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆCRUDï¼‰
- åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ»ç™»éŒ²
- æ¤œç´¢æ©Ÿèƒ½
"""

import os
import socket
import time
import logging
import traceback
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple, Iterable

import pandas as pd
import tiktoken
from helper_embedding import create_embedding_client, get_embedding_dimensions
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import UnexpectedResponse

logger = logging.getLogger(__name__)


# ===================================================================
# Qdrantè¨­å®š
# ===================================================================

QDRANT_CONFIG = {
    "name": "Qdrant",
    "host": "localhost",
    "port": 6333,
    "icon": "ğŸ¯",
    "url": "http://localhost:6333",
    "health_check_endpoint": "/collections",
    "docker_image": "qdrant/qdrant",
}

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å›ºæœ‰ã®åŸ‹ã‚è¾¼ã¿è¨­å®š
COLLECTION_EMBEDDINGS_SEARCH = {
    "qa_corpus": {"model": "gemini-embedding-001", "dims": 3072},
    "qa_cc_news_a02_llm": {"model": "gemini-embedding-001", "dims": 3072},
    "qa_cc_news_a03_rule": {"model": "gemini-embedding-001", "dims": 3072},
    "qa_cc_news_a10_hybrid": {"model": "gemini-embedding-001", "dims": 3072},
    "qa_livedoor_a02_20_llm": {"model": "gemini-embedding-001", "dims": 3072},
    "qa_livedoor_a03_rule": {"model": "gemini-embedding-001", "dims": 3072},
    "qa_livedoor_a10_hybrid": {"model": "gemini-embedding-001", "dims": 3072},
}

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å¯¾å¿œè¡¨
COLLECTION_CSV_MAPPING = {
    "qa_corpus": "qa_pairs_corpus.csv",
    "qa_cc_news_a02_llm": "a02_qa_pairs_cc_news.csv",
    "qa_cc_news_a03_rule": "a03_qa_pairs_cc_news.csv",
    "qa_cc_news_a10_hybrid": "a10_qa_pairs_cc_news.csv",
    "qa_livedoor_a02_20_llm": "a02_qa_pairs_livedoor.csv",
    "qa_livedoor_a03_rule": "a03_qa_pairs_livedoor.csv",
    "qa_livedoor_a10_hybrid": "a10_qa_pairs_livedoor.csv",
}


# ===================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ===================================================================

def batched(seq: Iterable, size: int):
    """ã‚¤ãƒ†ãƒ©ãƒ–ãƒ«ã‚’ãƒãƒƒãƒã«åˆ†å‰²"""
    buf = []
    for x in seq:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf


# ===================================================================
# Qdrantãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚«ãƒ¼
# ===================================================================

class QdrantHealthChecker:
    """Qdrantã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""

    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.client = None

    def check_port(self, host: str, port: int, timeout: float = 2.0) -> bool:
        """ãƒãƒ¼ãƒˆãŒé–‹ã„ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout)
            result = sock.connect_ex((host, port))
            sock.close()
            return result == 0
        except Exception as e:
            if self.debug_mode:
                logger.error(f"Port check failed for {host}:{port}: {e}")
            return False

    def check_qdrant(self) -> Tuple[bool, str, Optional[Dict]]:
        """Qdrantæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
        start_time = time.time()

        # ã¾ãšãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if not self.check_port(QDRANT_CONFIG["host"], QDRANT_CONFIG["port"]):
            return False, "Connection refused (port closed)", None

        try:
            self.client = QdrantClient(url=QDRANT_CONFIG["url"], timeout=5)

            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—
            collections = self.client.get_collections()

            metrics = {
                "collection_count": len(collections.collections),
                "collections": [c.name for c in collections.collections],
                "response_time_ms": round((time.time() - start_time) * 1000, 2),
            }

            return True, "Connected", metrics

        except Exception as e:
            error_msg = str(e)
            if self.debug_mode:
                error_msg = f"{error_msg}\n{traceback.format_exc()}"
            return False, error_msg, None


# ===================================================================
# Qdrantãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
# ===================================================================

class QdrantDataFetcher:
    """Qdrantã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""

    def __init__(self, client: QdrantClient):
        self.client = client

    def fetch_collections(self) -> pd.DataFrame:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—"""
        try:
            collections = self.client.get_collections()

            data = []
            for collection in collections.collections:
                try:
                    info = self.client.get_collection(collection.name)
                    data.append(
                        {
                            "Collection": collection.name,
                            "Vectors Count": info.vectors_count,
                            "Points Count": info.points_count,
                            "Indexed Vectors": info.indexed_vectors_count,
                            "Status": info.status,
                        }
                    )
                except Exception:
                    data.append(
                        {
                            "Collection": collection.name,
                            "Vectors Count": "N/A",
                            "Points Count": "N/A",
                            "Indexed Vectors": "N/A",
                            "Status": "Error",
                        }
                    )

            return (
                pd.DataFrame(data)
                if data
                else pd.DataFrame({"Info": ["No collections found"]})
            )

        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})

    def fetch_collection_points(
        self, collection_name: str, limit: int = 50
    ) -> pd.DataFrame:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points_result = self.client.scroll(
                collection_name=collection_name,
                limit=limit,
                with_payload=True,
                with_vectors=False,
            )

            points = points_result[0]  # scrollã¯ (points, next_offset) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™

            if not points:
                return pd.DataFrame({"Info": ["No points found in collection"]})

            # ãƒã‚¤ãƒ³ãƒˆã‚’DataFrameã«å¤‰æ›
            data = []
            for point in points:
                row = {"ID": point.id}

                # payloadã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åˆ—ã¨ã—ã¦è¿½åŠ 
                if point.payload:
                    for key, value in point.payload.items():
                        # é•·ã™ãã‚‹æ–‡å­—åˆ—ã¯åˆ‡ã‚Šè©°ã‚
                        if isinstance(value, str) and len(value) > 200:
                            row[key] = value[:200] + "..."
                        elif isinstance(value, (list, dict)):
                            row[key] = (
                                str(value)[:200] + "..."
                                if len(str(value)) > 200
                                else str(value)
                            )
                        else:
                            row[key] = value

                data.append(row)

            return pd.DataFrame(data)

        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})

    def fetch_collection_info(self, collection_name: str) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        try:
            collection_info = self.client.get_collection(collection_name)

            # configã®æ§‹é€ ã‚’å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹
            vector_config = collection_info.config.params.vectors

            # vector_configã®å‹ã‚’åˆ¤å®šã—ã¦é©åˆ‡ã«å‡¦ç†
            if hasattr(vector_config, "size"):
                # å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«è¨­å®š
                vector_size = vector_config.size
                distance = vector_config.distance
            elif hasattr(vector_config, "__iter__"):
                # Named vectorsè¨­å®šã®å ´åˆ
                vector_sizes = {}
                distances = {}
                for name, config in (
                    vector_config.items() if isinstance(vector_config, dict) else []
                ):
                    vector_sizes[name] = (
                        config.size if hasattr(config, "size") else "N/A"
                    )
                    distances[name] = (
                        config.distance if hasattr(config, "distance") else "N/A"
                    )
                vector_size = vector_sizes if vector_sizes else "N/A"
                distance = distances if distances else "N/A"
            else:
                vector_size = "N/A"
                distance = "N/A"

            return {
                "vectors_count": collection_info.vectors_count,
                "points_count": collection_info.points_count,
                "indexed_vectors": collection_info.indexed_vectors_count,
                "status": collection_info.status,
                "config": {
                    "vector_size": vector_size,
                    "distance": distance,
                },
            }
        except Exception as e:
            return {"error": str(e)}

    def fetch_collection_source_info(
        self, collection_name: str, sample_size: int = 200
    ) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—"""
        try:
            collection_info = self.client.get_collection(collection_name)
            total_points = collection_info.points_count

            # ã‚µãƒ³ãƒ—ãƒ«ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points_result = self.client.scroll(
                collection_name=collection_name,
                limit=min(sample_size, total_points),
                with_payload=True,
                with_vectors=False,
            )

            points = points_result[0]

            if not points:
                return {"total_points": total_points, "sources": {}, "sample_size": 0}

            # sourceã¨generation_methodã‚’é›†è¨ˆ
            source_stats = {}
            for point in points:
                if point.payload:
                    source = point.payload.get("source", "unknown")
                    method = point.payload.get("generation_method", "unknown")
                    domain = point.payload.get("domain", "unknown")

                    if source not in source_stats:
                        source_stats[source] = {
                            "sample_count": 0,
                            "method": method,
                            "domain": domain,
                        }
                    source_stats[source]["sample_count"] += 1

            # å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’æ¨å®š
            sample_total = len(points)
            for source, stats in source_stats.items():
                ratio = stats["sample_count"] / sample_total
                stats["estimated_total"] = int(total_points * ratio)
                stats["percentage"] = ratio * 100

            return {
                "total_points": total_points,
                "sources": source_stats,
                "sample_size": sample_total,
            }

        except Exception as e:
            return {"error": str(e)}


# ===================================================================
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†é–¢æ•°
# ===================================================================

def get_collection_stats(
    client: QdrantClient, collection_name: str
) -> Optional[Dict[str, Any]]:
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    try:
        collection_info = client.get_collection(collection_name)
        total_points = collection_info.points_count

        # ãƒ™ã‚¯ãƒˆãƒ«è¨­å®šæƒ…å ±ã‚’å–å¾—
        vectors_config = collection_info.config.params.vectors
        vector_info = {}

        if isinstance(vectors_config, dict):
            # Named Vectors
            for name, config in vectors_config.items():
                vector_info[name] = {
                    "size": config.size,
                    "distance": str(config.distance),
                }
        elif hasattr(vectors_config, "size"):
            # Single Vector
            vector_info["default"] = {
                "size": vectors_config.size,
                "distance": str(vectors_config.distance),
            }

        return {
            "total_points": total_points,
            "vector_config": vector_info,
            "status": collection_info.status,
        }

    except UnexpectedResponse as e:
        if "doesn't exist" in str(e) or "not found" in str(e).lower():
            return None
        raise
    except Exception as e:
        logger.error(f"çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_all_collections(client: QdrantClient) -> List[Dict[str, Any]]:
    """å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æƒ…å ±ã‚’å–å¾—"""
    collections = client.get_collections()
    collection_list = []

    for collection in collections.collections:
        try:
            info = client.get_collection(collection.name)
            collection_list.append(
                {
                    "name": collection.name,
                    "points_count": info.points_count,
                    "status": info.status,
                }
            )
        except Exception:
            collection_list.append(
                {"name": collection.name, "points_count": 0, "status": "unknown"}
            )

    return collection_list


def delete_all_collections(client: QdrantClient, excluded: List[str] = None) -> int:
    """å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤"""
    excluded = excluded or []
    collections = get_all_collections(client)

    if not collections:
        return 0

    to_delete = [c for c in collections if c["name"] not in excluded]

    if not to_delete:
        return 0

    deleted_count = 0
    failed_count = 0

    for col in to_delete:
        try:
            client.delete_collection(collection_name=col["name"])
            deleted_count += 1
        except Exception as e:
            logger.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {col['name']}: {e}")
            failed_count += 1

    return deleted_count


# ===================================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ç™»éŒ²é–¢æ•°
# ===================================================================

def load_csv_for_qdrant(
    path: str, required=("question", "answer"), limit: int = 0
) -> pd.DataFrame:
    """CSVã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆQdrantç™»éŒ²ç”¨ï¼‰"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"CSV not found: {path}")
    df = pd.read_csv(path)

    # åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
    column_mappings = {
        "Question": "question",
        "Response": "answer",
        "Answer": "answer",
        "correct_answer": "answer",
    }
    df = df.rename(columns=column_mappings)

    for col in required:
        if col not in df.columns:
            raise ValueError(
                f"{path} ã«ã¯ '{col}' åˆ—ãŒå¿…è¦ã§ã™ï¼ˆåˆ—: {list(df.columns)}ï¼‰"
            )

    df = df.fillna("").drop_duplicates(subset=list(required)).reset_index(drop=True)

    if limit and limit > 0:
        df = df.head(limit).copy()

    return df


def build_inputs_for_embedding(df: pd.DataFrame, include_answer: bool) -> List[str]:
    """åŸ‹ã‚è¾¼ã¿ç”¨å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    if include_answer:
        return (df["question"].astype(str) + "\n" + df["answer"].astype(str)).tolist()
    return df["question"].astype(str).tolist()


def embed_texts_for_qdrant(
    texts: List[str], model: str = "gemini-embedding-001", batch_size: int = 128
) -> List[List[float]]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†ã§Embeddingã«å¤‰æ›ï¼ˆGemini APIä½¿ç”¨ï¼‰"""
    # Gemini Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
    embedding_client = create_embedding_client(provider="gemini")
    dims = get_embedding_dimensions("gemini")  # 3072

    # ç©ºæ–‡å­—åˆ—ãƒ»ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚’é™¤å¤–
    valid_texts = []
    valid_indices = []
    for i, text in enumerate(texts):
        if text and text.strip():
            valid_texts.append(text)
            valid_indices.append(i)

    if not valid_texts:
        logger.warning("å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºæ–‡å­—åˆ—ã§ã™ã€‚ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã—ã¾ã™ã€‚")
        return [[0.0] * dims] * len(texts)

    # Gemini Embeddingã§ãƒãƒƒãƒå‡¦ç†
    valid_vecs = embedding_client.embed_texts(valid_texts, batch_size=batch_size)

    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åˆã‚ã›ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†é…ç½®
    vecs: List[List[float]] = []
    valid_vec_idx = 0
    for i in range(len(texts)):
        if i in valid_indices:
            vecs.append(valid_vecs[valid_vec_idx])
            valid_vec_idx += 1
        else:
            vecs.append([0.0] * dims)

    return vecs


def create_or_recreate_collection_for_qdrant(
    client: QdrantClient, name: str, recreate: bool, vector_size: int = 3072
):
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆã¾ãŸã¯å†ä½œæˆ"""
    vectors_config = models.VectorParams(
        size=vector_size, distance=models.Distance.COSINE
    )

    if recreate:
        try:
            client.delete_collection(collection_name=name)
        except Exception:
            pass
        client.create_collection(collection_name=name, vectors_config=vectors_config)
    else:
        try:
            client.get_collection(name)
        except Exception:
            client.create_collection(
                collection_name=name, vectors_config=vectors_config
            )

    # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ç´¢å¼•ã‚’ä½œæˆ
    try:
        client.create_payload_index(
            name, field_name="domain", field_schema=models.PayloadSchemaType.KEYWORD
        )
    except Exception:
        pass


def build_points_for_qdrant(
    df: pd.DataFrame, vectors: List[List[float]], domain: str, source_file: str
) -> List[models.PointStruct]:
    """Qdrantãƒã‚¤ãƒ³ãƒˆã‚’æ§‹ç¯‰"""
    n = len(df)
    if len(vectors) != n:
        raise ValueError(f"vectors length mismatch: df={n}, vecs={len(vectors)}")

    now_iso = datetime.now(timezone.utc).isoformat()
    points: List[models.PointStruct] = []

    for i, row in enumerate(df.itertuples(index=False)):
        payload = {
            "domain": domain,
            "question": getattr(row, "question"),
            "answer": getattr(row, "answer"),
            "source": os.path.basename(source_file),
            "created_at": now_iso,
            "schema": "qa:v1",
        }

        pid = abs(hash(f"{domain}-{source_file}-{i}")) & 0x7FFFFFFFFFFFFFFF
        points.append(models.PointStruct(id=pid, vector=vectors[i], payload=payload))

    return points


def upsert_points_to_qdrant(
    client: QdrantClient,
    collection: str,
    points: List[models.PointStruct],
    batch_size: int = 128,
) -> int:
    """ãƒã‚¤ãƒ³ãƒˆã‚’Qdrantã«ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ"""
    count = 0
    for chunk in batched(points, batch_size):
        client.upsert(collection_name=collection, points=chunk)
        count += len(chunk)
    return count


# ===================================================================
# æ¤œç´¢é–¢æ•°
# ===================================================================

def embed_query_for_search(
    query: str, model: str = "gemini-embedding-001", dims: Optional[int] = None
) -> List[float]:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆGemini APIä½¿ç”¨ï¼‰"""
    embedding_client = create_embedding_client(provider="gemini")
    return embedding_client.embed_text(query)


# ===================================================================
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆé–¢æ•°
# ===================================================================

def scroll_all_points_with_vectors(
    client: QdrantClient,
    collection_name: str,
    batch_size: int = 100,
    progress_callback: Optional[callable] = None,
) -> List[models.Record]:
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å…¨ãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«å«ã‚€ï¼‰ã‚’å–å¾—

    Args:
        client: QdrantClient
        collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        batch_size: 1å›ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§å–å¾—ã™ã‚‹ä»¶æ•°
        progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (å–å¾—æ¸ˆã¿ä»¶æ•°, ç·ä»¶æ•°)

    Returns:
        å…¨ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    all_points = []
    offset = None

    # ç·ä»¶æ•°ã‚’å–å¾—
    collection_info = client.get_collection(collection_name)
    total_points = collection_info.points_count

    while True:
        points, next_offset = client.scroll(
            collection_name=collection_name,
            limit=batch_size,
            offset=offset,
            with_payload=True,
            with_vectors=True,
        )

        if not points:
            break

        all_points.extend(points)

        if progress_callback:
            progress_callback(len(all_points), total_points)

        if next_offset is None:
            break

        offset = next_offset

    return all_points


def merge_collections(
    client: QdrantClient,
    source_collections: List[str],
    target_collection: str,
    recreate: bool = True,
    vector_size: int = 3072,
    progress_callback: Optional[callable] = None,
) -> Dict[str, Any]:
    """è¤‡æ•°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµ±åˆã—ã¦æ–°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç™»éŒ²

    Args:
        client: QdrantClient
        source_collections: çµ±åˆå…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã®ãƒªã‚¹ãƒˆ
        target_collection: çµ±åˆå…ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        recreate: æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¦å†ä½œæˆã™ã‚‹ã‹
        vector_size: ãƒ™ã‚¯ãƒˆãƒ«ã‚µã‚¤ã‚º
        progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸, ç¾åœ¨å€¤, æœ€å¤§å€¤)

    Returns:
        çµ±åˆçµæœã®è¾æ›¸
    """
    result = {
        "source_collections": source_collections,
        "target_collection": target_collection,
        "points_per_collection": {},
        "total_points": 0,
        "success": False,
        "error": None,
    }

    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: çµ±åˆå…ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        if progress_callback:
            progress_callback(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{target_collection}' ã‚’ä½œæˆä¸­...", 0, 100)

        create_or_recreate_collection_for_qdrant(
            client, target_collection, recreate, vector_size
        )

        # ã‚¹ãƒ†ãƒƒãƒ—2: å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—ã—ã¦çµ±åˆ
        all_points = []
        collection_count = len(source_collections)

        for idx, src_collection in enumerate(source_collections):
            if progress_callback:
                progress_callback(
                    f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{src_collection}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...",
                    int((idx / collection_count) * 50),
                    100,
                )

            # ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points = scroll_all_points_with_vectors(client, src_collection)
            result["points_per_collection"][src_collection] = len(points)

            # ãƒã‚¤ãƒ³ãƒˆIDã‚’å†ç”Ÿæˆï¼ˆé‡è¤‡å›é¿ï¼‰
            for i, point in enumerate(points):
                # å…ƒã®payloadã«ã‚½ãƒ¼ã‚¹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
                payload = dict(point.payload) if point.payload else {}
                payload["_source_collection"] = src_collection
                payload["_original_id"] = point.id

                # æ–°ã—ã„IDã‚’ç”Ÿæˆ
                new_id = abs(
                    hash(f"{target_collection}-{src_collection}-{point.id}-{i}")
                ) & 0x7FFFFFFFFFFFFFFF

                all_points.append(
                    models.PointStruct(
                        id=new_id,
                        vector=point.vector,
                        payload=payload,
                    )
                )

        result["total_points"] = len(all_points)

        # ã‚¹ãƒ†ãƒƒãƒ—3: çµ±åˆå…ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
        if progress_callback:
            progress_callback("çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆä¸­...", 50, 100)

        if all_points:
            upserted = 0
            batch_size = 128
            for chunk in batched(all_points, batch_size):
                client.upsert(collection_name=target_collection, points=chunk)
                upserted += len(chunk)
                if progress_callback:
                    progress = 50 + int((upserted / len(all_points)) * 50)
                    progress_callback(
                        f"ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆä¸­... ({upserted}/{len(all_points)})",
                        progress,
                        100,
                    )

        result["success"] = True

        if progress_callback:
            progress_callback("çµ±åˆå®Œäº†", 100, 100)

    except Exception as e:
        result["error"] = str(e)
        logger.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")

    return result