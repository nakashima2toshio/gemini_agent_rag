#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import os
import traceback
from typing import Dict, Any

# æ¨™æº–å‡ºåŠ›ã‚’å¼·åˆ¶çš„ã«ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

r"""
a02_make_qa_para.py - æ”¹å–„ç‰ˆQ/Aãƒšã‚¢è‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
(Refactored for direct import and execution)
"""

import json
import time
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import tiktoken
from helper_llm import create_llm_client, LLMClient
from dotenv import load_dotenv
import logging
import re
from collections import Counter

# ===================================================================
# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ===================================================================
from models import QAPairsResponse
from config import (
    DATASET_CONFIGS,
    QAGenerationConfig,
)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ========================================== 
# Import existing logic from original file
# (In a real refactor, we would import from the original file,
# but here we will duplicate the essential parts for safety/speed)
# ========================================== 
# Since we cannot easily import parts of a script without running it,
# we will assume the helper functions are available or we will redefine them.
# Ideally, we should have moved the logic to a separate module.
# For this fix, I will copy the ESSENTIAL functions needed for `run_qa_process`.
# However, to avoid massive code duplication in this `write_file` call,
# I will try to import `a02_make_qa_para` as a module if possible,
# but since it has a `main` execution block, we must be careful.
# The original `a02_make_qa_para.py` has `if __name__ == "__main__": main()`, 
# so it IS safe to import.

# We will wrap the main logic in a callable function `run_qa_generator`.

import a02_make_qa_para as original_script

def run_qa_generator(
    dataset: Optional[str] = None,
    input_file: Optional[str] = None,
    model: str = "gemini-2.0-flash",
    output_dir: str = "qa_output/a02",
    max_docs: Optional[int] = None,
    analyze_coverage: bool = False,
    batch_chunks: int = 3,
    merge_chunks: bool = True,
    min_tokens: int = 150,
    max_tokens: int = 400,
    use_celery: bool = False,
    celery_workers: int = 8,
    coverage_threshold: Optional[float] = None,
    log_callback=None
):
    """
    Q/Aç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼ˆç›´æ¥å‘¼ã³å‡ºã—ç”¨ï¼‰
    """
    # ãƒ­ã‚¬ãƒ¼ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’è¨­å®šã—ã¦callbackã«æµã™
    if log_callback:
        class CallbackHandler(logging.Handler):
            def emit(self, record):
                msg = self.format(record)
                log_callback(msg)
        
        handler = CallbackHandler()
        handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(handler)
        original_script.logger.addHandler(handler)

    try:
        logger.info("ğŸš€ Q/Aç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ (Direct Mode)")
        
        # APIã‚­ãƒ¼ç¢ºèª
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            logger.error("GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return {"success": False, "error": "GOOGLE_API_KEY missing"}

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        if input_file:
            dataset_type = "custom_upload"
            file_basename = Path(input_file).stem
            lang = "ja"
            config = {
                "name": f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« ({file_basename})",
                "text_column": "Combined_Text",
                "title_column": None,
                "lang": lang,
                "chunk_size": 300,
                "qa_per_chunk": 3,
            }
            dataset_name = config['name']
        else:
            dataset_type = dataset
            config = DATASET_CONFIGS[dataset_type]
            dataset_name = config['name']

        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")

        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("\n[1/4] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        if input_file:
            df = original_script.load_uploaded_file(input_file)
            if max_docs and len(df) > max_docs:
                df = df.head(max_docs)
                logger.info(f"  ğŸ“Š æœ€å¤§æ–‡æ›¸æ•°åˆ¶é™: {len(df)} ä»¶ã«åˆ¶é™")
        else:
            df = original_script.load_preprocessed_data(dataset_type)

        # 2. ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        logger.info("\n[2/4] ãƒãƒ£ãƒ³ã‚¯ä½œæˆ...")
        max_docs_for_chunks = None if input_file else max_docs
        chunks = original_script.create_document_chunks(df, dataset_type, max_docs_for_chunks, config=config)

        if not chunks:
            logger.error("ãƒãƒ£ãƒ³ã‚¯ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return {"success": False, "error": "No chunks created"}

        # 3. Q/Aãƒšã‚¢ç”Ÿæˆ
        logger.info("\n[3/4] Q/Aãƒšã‚¢ç”Ÿæˆ...")
        qa_pairs = []

        if use_celery:
            # Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®äº‹å‰ç¢ºèª
            logger.info("Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
            if not original_script.check_celery_workers(celery_workers):
                logger.error("Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
                return {"success": False, "error": "Celery workers not ready"}
            logger.info(f"âœ“ Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ç¢ºèªOKï¼ˆ{celery_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰")

            # Celeryã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from celery_tasks import submit_unified_qa_generation, collect_results

            # ãƒãƒ£ãƒ³ã‚¯ã®å‰å‡¦ç†
            if merge_chunks:
                processed_chunks = original_script.merge_small_chunks(chunks, min_tokens, max_tokens)
            else:
                processed_chunks = chunks

            # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯æŠ•å…¥
            tasks = submit_unified_qa_generation(
                processed_chunks, config, model, provider="gemini"
            )

            timeout_seconds = min(max(len(tasks) * 10, 600), 1800)
            logger.info(f"çµæœåé›†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout_seconds}ç§’ï¼ˆ{len(tasks)}ã‚¿ã‚¹ã‚¯ï¼‰")
            qa_pairs = collect_results(tasks, timeout=timeout_seconds)
        else:
            logger.info("é€šå¸¸å‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
            qa_pairs = original_script.generate_qa_for_dataset(
                chunks,
                dataset_type,
                model,
                chunk_batch_size=batch_chunks,
                merge_chunks=merge_chunks,
                min_tokens=min_tokens,
                max_tokens=max_tokens,
                config=config
            )

        if not qa_pairs:
            logger.warning("Q/Aãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        # 4. ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
        coverage_results = {}
        if analyze_coverage and qa_pairs:
            logger.info("\n[4/4] ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆEmbeddingç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰...")
            coverage_results = original_script.analyze_coverage(
                chunks, qa_pairs, dataset_type,
                custom_threshold=coverage_threshold
            )
            logger.info(f"ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡: {coverage_results.get('coverage_rate', 0):.1%}")

        # 5. çµæœä¿å­˜
        logger.info("\nçµæœã‚’ä¿å­˜ä¸­...")
        saved_files = original_script.save_results(qa_pairs, coverage_results, dataset_type, output_dir)

        return {
            "success": True,
            "saved_files": saved_files,
            "qa_count": len(qa_pairs),
            "coverage_results": coverage_results
        }

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        # ãƒãƒ³ãƒ‰ãƒ©ã®å‰Šé™¤
        if log_callback:
            logger.removeHandler(handler)
            original_script.logger.removeHandler(handler)

if __name__ == "__main__":
    # Test run
    pass
