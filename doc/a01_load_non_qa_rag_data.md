# a01_load_non_qa_rag_data.py ドキュメント

作成日: 2025-11-28 (最終更新: Gemini移行対応)

本ドキュメントは、非Q&A型RAGデータ処理ツール（`a01_load_non_qa_rag_data.py`）の機能、操作方法、およびGemini APIへの対応について解説します。

## 目次

- [1. 概要](#1-概要)
  - [1.1 ツール概要](#11-ツール概要)
  - [1.2 主要機能](#12-主要機能)
  - [1.3 対応データセット](#13-対応データセット)
- [2. 画面構成と基本操作](#2-画面構成と基本操作)
  - [2.1 サイドバー設定](#21-サイドバー設定)
  - [2.2 メインコンテンツのタブ](#22-メインコンテンツのタブ)
- [3. 📁 データアップロード](#3--データアップロード)
  - [3.1 ファイルアップロード](#31-ファイルアップロード)
  - [3.2 HuggingFaceからの自動ロード](#32-huggingfaceからの自動ロード)
- [4. 🔍 データ検証](#4--データ検証)
  - [4.1 基本検証](#41-基本検証)
  - [4.2 データセット固有の検証](#42-データセット固有の検証)
- [5. ⚙️ 前処理実行](#5--前処理実行)
  - [5.1 前処理オプション](#51-前処理オプション)
  - [5.2 トークン使用量推定（Gemini API利用）](#52-トークン使用量推定gemini-api利用)
  - [5.3 テキスト長の分布](#53-テキスト長の分布)
- [6. 📊 結果・ダウンロード](#6--結果ダウンロード)
  - [6.1 処理サマリー](#61-処理サマリー)
  - [6.2 ファイルダウンロード](#62-ファイルダウンロード)
  - [6.3 OUTPUTフォルダへの保存](#63-outputフォルダへの保存)
- [7. データセット固有の検証関数](#7-データセット固有の検証関数)
  - [7.1 validate_wikipedia_data_specific()](#71-validate_wikipedia_data_specific)
  - [7.2 validate_news_data_specific()](#72-validate_news_data_specific)
- [8. Livedoorコーパス用関数](#8-livedoorコーパス用関数)
  - [8.1 download_livedoor_corpus()](#81-download_livedoor_corpus)
  - [8.2 load_livedoor_corpus()](#82-load_livedoor_corpus)
- [9. トラブルシューティング](#9-トラブルシューティング)

---

## 1. 概要

### 1.1 ツール概要

`a01_load_non_qa_rag_data.py` は、RAGシステムで使用する非Q&A形式のデータを効率的に処理するためのツールです。HuggingFaceデータセットやローカルファイルを読み込み、RAGに最適化された形式に前処理し、トークン使用量を推定します。

### 1.2 主要機能

| 機能 | 説明 |
|---|---|
| **日本語・英語データセット処理** | Wikipedia日本語版、CC100日本語、CC-News英語、Livedoorニュースコーパスなどに対応 |
| **データ検証・品質チェック** | 必須カラムチェック、空値・重複行検出、データセット固有の検証 |
| **RAG用テキスト抽出・前処理** | タイトルと本文の結合、改行・空白・引用符の正規化、短いテキスト・重複の除去 |
| **トークン使用量推定** | 選択したGemini LLMモデルに基づく正確なトークン数と推定コストの表示 |
| **各種フォーマット出力** | 処理済みデータをCSV, TXT, JSON形式でダウンロード/保存 |

### 1.3 対応データセット

1.  `wikipedia_ja`: Wikipedia日本語版（百科事典的知識）
2.  `japanese_text`: CC100日本語（Webテキストコーパス）
3.  `cc_news`: CC-News（英語ニュース記事）
4.  `livedoor`: Livedoorニュースコーパス（日本語ニュース9カテゴリ）

---

## 2. 画面構成と基本操作

`a01_load_non_qa_rag_data.py` はStreamlitアプリケーションとして動作し、以下の要素で構成されます。

### 2.1 サイドバー設定

*   **📊 データセットタイプ選択**: 処理するデータセットのタイプを選択します。
*   **🤖 モデルを選択**: RAG用途で使用するGemini LLMモデルを選択します。
*   **📊 選択モデル情報**: 選択したモデルの最大トークン数、料金、RAG用途推奨度などが表示されます。
*   **⚙️ データセット固有設定**: 選択したデータセットに応じて、Wikiマークアップ除去、URL除去、最小テキスト長などのオプションを調整します。

### 2.2 メインコンテンツのタブ

以下の4つのタブで構成され、データ処理の各ステップを進めます。

*   **📁 データアップロード**: ローカルからのファイルアップロード、またはHuggingFaceからの自動ロード。
*   **🔍 データ検証**: データの品質、必須フィールド、データセット固有の特性をチェック。
*   **⚙️ 前処理実行**: RAGに最適化するためのテキスト抽出、クレンジング、フィルタリングを実行。
*   **📊 結果・ダウンロード**: 処理結果のサマリー表示、各種フォーマットでのダウンロード/保存。

---

## 3. 📁 データアップロード

### 3.1 ファイルアップロード

CSVファイルを直接アップロードして処理を開始できます。

### 3.2 HuggingFaceからの自動ロード

事前定義されたHuggingFaceデータセットを直接ダウンロードしてロードします。LivedoorニュースコーパスはURLからのダウンロードに対応しています。

**推奨設定の表示:**
- データセット名、Config、Split、サンプル数などが自動表示されます。
- `st.progress` を使用したダウンロード進捗バーが表示されます。

---

## 4. 🔍 データ検証

### 4.1 基本検証

`helper_rag.py` の `validate_data` 関数が以下の基本チェックを実行します。

-   総行数、総列数
-   必須列の有無
-   各列の空値数
-   重複行の有無

### 4.2 データセット固有の検証

データセットのタイプに応じて、以下のような特有の検証が実行されます。

*   **Wikipedia**: 平均テキスト長、Wikiマークアップの有無、重複タイトル。
*   **ニュースデータ (CC100日本語, CC-News英語, Livedoor)**: 平均記事長、短い記事の割合、カテゴリ分布。

---

## 5. ⚙️ 前処理実行

### 5.1 前処理オプション

*   **短いテキストを除外**: 指定した最小文字数未満のテキストを除外します。
*   **重複を除去**: 完全に同じテキスト内容を持つレコードを除外します。

これらのオプションは `helper_rag.py` の `process_rag_data` 関数で処理されます。

### 5.2 トークン使用量推定（Gemini API利用）

処理済みの `Combined_Text` カラムに基づいて、選択されたGemini LLMモデルでのトークン使用量を推定します。
`helper_rag.py` の `estimate_token_usage` 関数が、`UnifiedLLMClient.count_tokens` および Gemini Embeddingの料金情報を使用して、LLMとEmbeddingの両方の推定コストを算出します。

*   推定総トークン数
*   平均トークン/レコード
*   推定Embedding費用

### 5.3 テキスト長の分布

処理後のテキストの文字数分布（平均、最小、最大、中央値、四分位数）が表示され、データセットの特性を把握できます。

---

## 6. 📊 結果・ダウンロード

### 6.1 処理サマリー

処理された件数、除外された件数、残存率などが表示されます。

### 6.2 ファイルダウンロード

処理済みのデータを以下の形式でダウンロードできます。

*   📄 CSVファイル (`preprocessed_{dataset_type}.csv`)
*   📝 テキストファイル (`{dataset_type}.txt`) 
*   📋 メタデータ(JSON) (`metadata_{dataset_type}.json`)

### 6.3 OUTPUTフォルダへの保存

**「💾 OUTPUTフォルダに保存」** ボタンをクリックすると、処理済みのCSV, TXT, メタデータJSONファイルがローカルの `OUTPUT/` ディレクトリに保存されます。

---

## 7. データセット固有の検証関数

`a01_load_non_qa_rag_data.py` の中で、特定のデータセットに特化した検証ロジックが実装されています。

### 7.1 validate_wikipedia_data_specific()

Wikipediaデータに対して、Wikiマークアップの有無や平均テキスト長、重複タイトルなどをチェックします。

### 7.2 validate_news_data_specific()

ニュースデータ（CC100日本語, CC-News英語, Livedoor）に対して、平均記事長、短い記事の割合、カテゴリ分布などを分析します。

---

## 8. Livedoorコーパス用関数

### 8.1 download_livedoor_corpus()

LivedoorニュースコーパスをURLからダウンロードし、指定ディレクトリに解凍します。セキュリティのため、`tar.extractall` に `filter='data'` を適用します。

### 8.2 load_livedoor_corpus()

解凍されたLivedoorコーパスのファイル群から記事を読み込み、URL、日付、タイトル、本文、カテゴリを抽出してDataFrameを構築します。

---

## 9. トラブルシューティング

| エラー内容 | 主な原因 | 対処法 |
|---|---|---|
| `GOOGLE_API_KEY`が設定されていません | 環境変数 `GOOGLE_API_KEY` が設定されていない | `.env`ファイルに `GOOGLE_API_KEY=your_api_key_here` を設定する |
| HuggingFaceデータセットのロードエラー | データセット名、config名、split名が誤っている | HuggingFaceの公式ページで正しい情報を確認する |
| メモリ不足 | 大量のデータを一度に処理している | サンプル数を減らす、Streamlitアプリを再起動する |
| `MeCab`が見つかりません | MeCabがインストールされていない、またはパスが通っていない | MeCabをインストールし、`mecab-python3`を`pip install`する |

