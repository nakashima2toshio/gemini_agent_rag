#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import os
# æ¨™æº–å‡ºåŠ›ã‚’å¼·åˆ¶çš„ã«ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)
print("DEBUG: a02_make_qa_para.py script started (reconfigured stdout)", flush=True)
r"""
a02_make_qa_para.py - æ”¹å–„ç‰ˆQ/Aãƒšã‚¢è‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
=========================================================================
OUTPUTãƒ•ã‚©ãƒ«ãƒ€å†…ã®preprocessedãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é«˜å“è³ªãªQ/Aãƒšã‚¢ã‚’è‡ªå‹•ç”Ÿæˆ
ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–ã§APIå‘¼ã³å‡ºã—å›æ•°ã‚’å¤§å¹…å‰Šæ¸›ï¼ˆæœ€å¤§1/5ï¼‰
Celeryã«ã‚ˆã‚‹éåŒæœŸä¸¦åˆ—å‡¦ç†ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆè¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã§åŒæ™‚å®Ÿè¡Œå¯èƒ½ï¼‰

ä¸»è¦æ©Ÿèƒ½:
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ã«ã‚ˆã‚‹ãƒãƒ£ãƒ³ã‚¯ä½œæˆï¼ˆæ®µè½å¢ƒç•Œã‚’å„ªå…ˆï¼‰
- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹ä¸¦åˆ—Q/Aç”Ÿæˆï¼ˆ1-5ãƒãƒ£ãƒ³ã‚¯åŒæ™‚å‡¦ç†ï¼‰
- Celeryã«ã‚ˆã‚‹éåŒæœŸä¸¦åˆ—å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- å°ãƒãƒ£ãƒ³ã‚¯è‡ªå‹•çµ±åˆã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
- å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æï¼ˆstrict/standard/lenientï¼‰
- ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æï¼ˆé•·ã•åˆ¥ãƒ»ä½ç½®åˆ¥ï¼‰

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„æ©Ÿèƒ½ï¼ˆ2024å¹´11æœˆè¿½åŠ ï¼‰:
- è³ªå•ã‚¿ã‚¤ãƒ—ã®éšå±¤åŒ–ï¼ˆåŸºç¤/ç†è§£/å¿œç”¨ã®3éšå±¤ã€è¨ˆ11ã‚¿ã‚¤ãƒ—ï¼‰
- ãƒãƒ£ãƒ³ã‚¯è¤‡é›‘åº¦åˆ†æã«ã‚ˆã‚‹å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»å°‚é–€ç”¨èªã®è‡ªå‹•æŠ½å‡ºã¨æ´»ç”¨
- é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«ï¼ˆeasy/medium/hardï¼‰ã®è‡ªå‹•åˆ¤å®š
- å“è³ªã‚¹ã‚³ã‚¢ã¨ç¢ºä¿¡åº¦ã‚¹ã‚³ã‚¢ã®ä»˜ä¸

å¯¾å¿œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:
- cc_news: CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆ7,376ä»¶ï¼‰
- japanese_text: æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ
- wikipedia_ja: Wikipediaæ—¥æœ¬èªç‰ˆ
- livedoor: Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆ7,376ä»¶ï¼‰

========================================
å®Ÿè¡Œæ–¹æ³•
========================================
ä½¿ç”¨æ‰‹é †

1. Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®ç¢ºèªã¨å†èµ·å‹•
# ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‹•ä½œç¢ºèª
python test_celery.py
ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
2. å°‘æ•°ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰
# 1. Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å†èµ·å‹•
  redis-cli FLUSHDB && ./start_celery.sh restart -w 8

# 2. ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆ1ãƒãƒ£ãƒ³ã‚¯ã®ã¿ï¼‰
python a02_make_qa_para.py --dataset cc_news --use-celery --celery-workers 8 --batch-chunks 1 --max-docs 1 --model gemini-2.0-flash

python a02_make_qa_para.py \
  --dataset cc_news \
  --use-celery \
  --celery-workers 8 \
  --batch-chunks 3 \
  --merge-chunks \
  --min-tokens 150 \
  --max-tokens 400 \
  --coverage-threshold 0.58 \
  --model gemini-2.0-flash \
  --analyze-coverage


python a02_make_qa_para.py \
  --input-file qa_output/qa_pairs_upload_20251122_182355.csv\
  --use-celery \
  --celery-workers 8 \
  --batch-chunks 3 \
  --max-docs 10 \
  --merge-chunks \
  --min-tokens 150 \
  --max-tokens 400 \
  --coverage-threshold 0.58 \
  --model gemini-2.0-flash \
  --analyze-coverage

ãƒ¼ãƒ¼ãƒ¼ãƒ¼
æ¨å¥¨ã‚³ãƒãƒ³ãƒ‰ï¼ˆGemini APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–æ¸ˆã¿ï¼‰

python a02_make_qa_para.py \
  --dataset cc_news \
  --use-celery \
  --celery-workers 8 \
  --batch-chunks 3 \
  --merge-chunks \
  --min-tokens 150 \
  --max-tokens 400 \
  --coverage-threshold 0.58 \
  --model gemini-2.0-flash \
  --analyze-coverage

3. å•é¡Œè¨ºæ–­

å®Ÿè¡Œæ™‚ã«ä»¥ä¸‹ã®ã‚ˆã†ãªé€²æ—ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼š
é€²æ—: æˆåŠŸ=3/17, å¤±æ•—=0, å®Ÿè¡Œä¸­=4, å¾…æ©Ÿä¸­=10, çµŒéæ™‚é–“=15.2ç§’
é€²æ—: æˆåŠŸ=7/17, å¤±æ•—=0, å®Ÿè¡Œä¸­=4, å¾…æ©Ÿä¸­=6, çµŒéæ™‚é–“=20.4ç§’

4. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå‹•ä½œã—ãªã„å ´åˆï¼š
# ãƒ­ã‚°ã‚’ç¢ºèª
tail -f logs/celery_qa_*.log

# ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¢ºèª
ps aux | grep celery

# Redisã®çŠ¶æ…‹ç¢ºèª
redis-cli INFO clients

ã‚¿ã‚¹ã‚¯ãŒå‡¦ç†ã•ã‚Œãªã„å ´åˆï¼š
# ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
redis-cli LLEN celery

# ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å†èµ·å‹•ï¼ˆGeminiæ¨å¥¨ä¸¦åˆ—åº¦ï¼‰
./start_celery.sh restart -w 8


===================================


## 1. é€šå¸¸å®Ÿè¡Œï¼ˆåŒæœŸå‡¦ç†ï¼‰
python a02_make_qa_para.py \
    --dataset livedoor \
    --batch-chunks 3 \
    --merge-chunks \
    --min-tokens 100 \
    --max-tokens 300 \
    --model gemini-2.0-flash \
    --max-docs 20 \
    --analyze-coverage

## 2. Celeryä¸¦åˆ—å®Ÿè¡Œï¼ˆéåŒæœŸå‡¦ç†ï¼‰

### æº–å‚™æ‰‹é †:
# 1. Redisã‚µãƒ¼ãƒãƒ¼èµ·å‹•
brew services start redis  # macOS
# ã¾ãŸã¯
redis-server                # Linux/æ‰‹å‹•èµ·å‹•

# 2. Celeryãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•
./start_celery.sh start -w 8  # 8ãƒ¯ãƒ¼ã‚«ãƒ¼ã§èµ·å‹•

### å®Ÿè¡Œ:
# 3. ä¸¦åˆ—å®Ÿè¡Œï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã¯å‰Šé™¤ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰
python a02_make_qa_para.py \
    --dataset livedoor \
    --use-celery \
    --celery-workers 8 \
    --batch-chunks 3 \
    --merge-chunks \
    --min-tokens 100 \
    --max-tokens 300 \
    --model gemini-2.0-flash \
    --max-docs 20 \
    --analyze-coverage

### ç®¡ç†:
# 4. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
./start_celery.sh status

# 5. ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢
./start_celery.sh stop

========================================
å®Ÿè¡Œæ™‚é–“ã®è¦‹ç©ã‚‚ã‚Š

| é …ç›®      | å€¤                     |
|---------|-----------------------|
| å‡¦ç†æ–‡æ›¸æ•°   | 497ä»¶ï¼ˆå…¨ä»¶ï¼‰              |
| ãƒãƒ£ãƒ³ã‚¯æ•°   | ~1,825å€‹ â†’ çµ±åˆå¾Œ ~1,820å€‹ |
| APIå‘¼ã³å‡ºã— | ç´„365å›ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º5ï¼‰        |
| æ¨å®šå®Ÿè¡Œæ™‚é–“  | 60-75åˆ†                |
| ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ | +3-5åˆ†                 |
| åˆè¨ˆ      | ç´„65-80åˆ†               |


  ç”ŸæˆQ/Aãƒšã‚¢æ•°: 525
  ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
  - Q/A (JSON): qa_output/qa_pairs_cc_news_20251020_143052.json
  - Q/A (CSV): qa_output/qa_pairs_cc_news_20251020_143052.csv
  - ã‚«ãƒãƒ¬ãƒ¼ã‚¸: qa_output/coverage_cc_news_20251020_143052.json
  - ã‚µãƒãƒªãƒ¼: qa_output/summary_cc_news_20251020_143052.json
    â€¢ æ–‡æ›¸å¾ŒåŠéƒ¨åˆ†ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒã‚„ã‚„ä½ã„ï¼ˆ85.0%ï¼‰
    â€¢ Shortãƒãƒ£ãƒ³ã‚¯ã§è¿½åŠ Q/Aç”Ÿæˆã®ä½™åœ°ã‚ã‚Š
 ----------------------------------------------------------
    python a02_make_qa_para.py --dataset cc_news --batch-chunks 5 --merge-chunks --analyze-coverage

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆæ¨å¥¨: gemini-2.0-flashãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
    python a02_make_qa_para.py --dataset cc_news --model gemini-2.0-flash --batch-chunks 5  --analyze-coverage --max-docs 10
    python a02_make_qa_para.py --dataset wikipedia_ja --model gemini-2.0-flash  --analyze-coverage --max-docs 10
    python a02_make_qa_para.py --dataset japanese_text --model gemini-2.0-flash  --analyze-coverage --max-docs 10
    python a02_make_qa_para.py --dataset livedoor --model gemini-2.0-flash --analyze-coverage --max-docs 100
"""

import os
import sys
import json
import time
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import tiktoken
from helper_llm import create_llm_client, LLMClient
from dotenv import load_dotenv
import logging
import re
from collections import Counter

# ===================================================================
# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ===================================================================
from models import QAPairsResponse
from config import (
    DATASET_CONFIGS,
    QAGenerationConfig,
)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==========================================
# è³ªå•ã‚¿ã‚¤ãƒ—éšå±¤æ§‹é€ ï¼ˆconfig.pyã‹ã‚‰å‚ç…§ï¼‰
# ==========================================
QUESTION_TYPES_HIERARCHY = QAGenerationConfig.QUESTION_TYPES_HIERARCHY


# ==========================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®æ‹¡å¼µï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å›ºæœ‰è¨­å®šï¼‰
# ==========================================
# DATASET_CONFIGS ã¯ config.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¸ˆã¿
# ä»¥ä¸‹ã¯ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆå›ºæœ‰ã®æ‹¡å¼µè¨­å®š

# config.pyã®è¨­å®šã‚’æ‹¡å¼µï¼ˆè¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
_LOCAL_DATASET_EXTENSIONS = {
    "cc_news": {
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en",
    },
    "japanese_text": {
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja",
    },
    "wikipedia_ja": {
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja",
    },
    "livedoor": {
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja",
    }
}

# DATASET_CONFIGSã«ãƒ­ãƒ¼ã‚«ãƒ«æ‹¡å¼µã‚’è¿½åŠ 
for dataset_name, extensions in _LOCAL_DATASET_EXTENSIONS.items():
    if dataset_name in DATASET_CONFIGS:
        DATASET_CONFIGS[dataset_name].update(extensions)


# ==========================================
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¯ãƒ©ã‚¹ï¼ˆMeCab/æ­£è¦è¡¨ç¾è‡ªå‹•åˆ‡æ›¿ï¼‰
# ==========================================

class KeywordExtractor:
    """
    MeCabã¨æ­£è¦è¡¨ç¾ã‚’çµ±åˆã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¯ãƒ©ã‚¹
    MeCabãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯è¤‡åˆåè©æŠ½å‡ºã‚’å„ªå…ˆã—ã€
    åˆ©ç”¨ä¸å¯ã®å ´åˆã¯æ­£è¦è¡¨ç¾ç‰ˆã«è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """

    def __init__(self, prefer_mecab: bool = True):
        """
        Args:
            prefer_mecab: MeCabã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        """
        self.prefer_mecab = prefer_mecab
        self.mecab_available = self._check_mecab_availability()

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©
        self.stopwords = {
            'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ãŸã‚', 'ã‚ˆã†', 'ã•ã‚“',
            'ã¾ã™', 'ã§ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã§ãã‚‹',
            'ã„ã†', 'çš„', 'ãª', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã§', 'ã¨',
            'ã®', 'ã‹ã‚‰', 'ã¾ã§', 'ç­‰', 'ãªã©', 'ã‚ˆã‚‹', 'ãŠã', 'ãã‚‹'
        }

        if self.mecab_available:
            logger.info("âœ… MeCabãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼ˆè¤‡åˆåè©æŠ½å‡ºãƒ¢ãƒ¼ãƒ‰ï¼‰")
        else:
            logger.info("âš ï¸ MeCabãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆæ­£è¦è¡¨ç¾ãƒ¢ãƒ¼ãƒ‰ï¼‰")

    def _check_mecab_availability(self) -> bool:
        """MeCabã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            import MeCab
            # å®Ÿéš›ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦å‹•ä½œç¢ºèª
            tagger = MeCab.Tagger()
            tagger.parse("ãƒ†ã‚¹ãƒˆ")
            return True
        except (ImportError, RuntimeError):
            return False

    def extract(self, text: str, top_n: int = 5) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆè‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰

        Args:
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            top_n: æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°

        Returns:
            ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        """
        if self.mecab_available and self.prefer_mecab:
            try:
                keywords = self._extract_with_mecab(text, top_n)
                if keywords:  # ç©ºã§ãªã‘ã‚Œã°æˆåŠŸ
                    return keywords
            except Exception as e:
                logger.warning(f"âš ï¸ MeCabæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ­£è¦è¡¨ç¾ç‰ˆ
        return self._extract_with_regex(text, top_n)

    def _extract_with_mecab(self, text: str, top_n: int) -> List[str]:
        """MeCabã‚’ä½¿ç”¨ã—ãŸè¤‡åˆåè©æŠ½å‡º"""
        import MeCab

        tagger = MeCab.Tagger()
        node = tagger.parseToNode(text)

        # è¤‡åˆåè©ã®æŠ½å‡º
        compound_buffer = []
        compound_nouns = []

        while node:
            features = node.feature.split(',')
            pos = features[0]  # å“è©

            if pos == 'åè©':
                compound_buffer.append(node.surface)
            else:
                # åè©ä»¥å¤–ãŒæ¥ãŸã‚‰ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                if compound_buffer:
                    compound_noun = ''.join(compound_buffer)
                    if len(compound_noun) > 0:
                        compound_nouns.append(compound_noun)
                    compound_buffer = []

            node = node.next

        # æœ€å¾Œã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
        if compound_buffer:
            compound_noun = ''.join(compound_buffer)
            if len(compound_noun) > 0:
                compound_nouns.append(compound_noun)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        return self._filter_and_count(compound_nouns, top_n)

    def _extract_with_regex(self, text: str, top_n: int) -> List[str]:
        """æ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        # ã‚«ã‚¿ã‚«ãƒŠèªã€æ¼¢å­—è¤‡åˆèªã€è‹±æ•°å­—ã‚’æŠ½å‡º
        pattern = r'[ã‚¡-ãƒ´ãƒ¼]{2,}|[ä¸€-é¾¥]{2,}|[A-Za-z]{2,}[A-Za-z0-9]*'
        words = re.findall(pattern, text)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        return self._filter_and_count(words, top_n)

    def _filter_and_count(self, words: List[str], top_n: int) -> List[str]:
        """é »åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
        filtered = [w for w in words if w not in self.stopwords and len(w) > 1]

        # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        word_freq = Counter(filtered)

        # ä¸Šä½Nä»¶ã‚’è¿”ã™
        return [word for word, freq in word_freq.most_common(top_n)]


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªKeywordExtractorã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆä¸€åº¦ã ã‘åˆæœŸåŒ–ï¼‰
_keyword_extractor = None

def get_keyword_extractor() -> KeywordExtractor:
    """KeywordExtractorã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _keyword_extractor
    if _keyword_extractor is None:
        _keyword_extractor = KeywordExtractor()
    return _keyword_extractor


# ==========================================
# è¤‡é›‘åº¦åˆ†æã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
# ==========================================

def analyze_chunk_complexity(chunk_text: str, lang: str = "ja") -> Dict:
    """ãƒãƒ£ãƒ³ã‚¯ã®è¤‡é›‘åº¦ã‚’åˆ†æ

    Args:
        chunk_text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        lang: è¨€èª

    Returns:
        è¤‡é›‘åº¦æŒ‡æ¨™ã®è¾æ›¸
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")

    # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    sentences = chunk_text.split('ã€‚') if lang == 'ja' else chunk_text.split('.')
    tokens = tokenizer.encode(chunk_text)

    # å°‚é–€ç”¨èªã®æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
    if lang == 'ja':
        # ã‚«ã‚¿ã‚«ãƒŠèªã€æ¼¢å­—è¤‡åˆèªã‚’å°‚é–€ç”¨èªå€™è£œã¨ã™ã‚‹
        technical_pattern = r'[ã‚¡-ãƒ´ãƒ¼]{4,}|[ä¸€-é¾¥]{4,}'
        technical_terms = re.findall(technical_pattern, chunk_text)
    else:
        # å¤§æ–‡å­—ã§å§‹ã¾ã‚‹è¤‡åˆèªã€é•·ã„å˜èªã‚’å°‚é–€ç”¨èªå€™è£œã¨ã™ã‚‹
        technical_pattern = r'[A-Z][a-z]+(?:[A-Z][a-z]+)+|\b\w{10,}\b'
        technical_terms = re.findall(technical_pattern, chunk_text)

    # æ–‡ã®è¤‡é›‘åº¦ï¼ˆå¹³å‡æ–‡é•·ï¼‰
    avg_sentence_length = len(tokens) / max(len(sentences), 1)

    # æ¦‚å¿µå¯†åº¦ï¼ˆå°‚é–€ç”¨èªã®é »åº¦ï¼‰
    concept_density = len(technical_terms) / max(len(tokens), 1) * 100

    # è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š
    if concept_density > 5 or avg_sentence_length > 30:
        complexity_level = "high"
    elif concept_density > 2 or avg_sentence_length > 20:
        complexity_level = "medium"
    else:
        complexity_level = "low"

    return {
        "complexity_level": complexity_level,
        "technical_terms": list(set(technical_terms))[:10],  # ä¸Šä½10å€‹
        "avg_sentence_length": avg_sentence_length,
        "concept_density": concept_density,
        "sentence_count": len(sentences),
        "token_count": len(tokens)
    }

def extract_key_concepts(chunk_text: str, lang: str = "ja", top_n: int = 5) -> List[str]:
    """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡º

    Args:
        chunk_text: ãƒ†ã‚­ã‚¹ãƒˆ
        lang: è¨€èª
        top_n: æŠ½å‡ºã™ã‚‹æ¦‚å¿µæ•°

    Returns:
        ä¸»è¦æ¦‚å¿µã®ãƒªã‚¹ãƒˆ
    """
    # KeywordExtractorã‚’ä½¿ç”¨
    extractor = get_keyword_extractor()
    keywords = extractor.extract(chunk_text, top_n=top_n)

    # è¤‡é›‘åº¦åˆ†æã‹ã‚‰å°‚é–€ç”¨èªã‚‚è¿½åŠ 
    complexity = analyze_chunk_complexity(chunk_text, lang)
    technical_terms = complexity.get("technical_terms", [])

    # é‡è¤‡ã‚’é™¤ã„ã¦çµ±åˆ
    all_concepts = list(set(keywords + technical_terms[:3]))

    return all_concepts[:top_n]

# ==========================================
# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯ä½œæˆé–¢æ•°
# ==========================================

def create_semantic_chunks(text: str, lang: str = "ja", max_tokens: int = 200, chunk_id_prefix: str = "chunk") -> List[Dict]:
    """
    ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ã«ã‚ˆã‚‹ãƒãƒ£ãƒ³ã‚¯ä½œæˆï¼ˆæ®µè½å„ªå…ˆï¼‰

    helper_rag_qa.pyã®SemanticCoverage.create_semantic_chunks()ã‚’ä½¿ç”¨ã—ã€
    æ®µè½å¢ƒç•Œã‚’æœ€å„ªå…ˆã—ãŸã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ã‚’å®Ÿè¡Œã€‚
    æ–‡è„ˆã‚’ä¿æŒã—ãªãŒã‚‰é©åˆ‡ãªã‚µã‚¤ã‚ºã§ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã€‚

    Args:
        text: åˆ†å‰²å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        lang: è¨€èªï¼ˆ"ja" or "en"ï¼‰â€»ç¾åœ¨ã¯è‡ªå‹•åˆ¤å®š
        max_tokens: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        chunk_id_prefix: ãƒãƒ£ãƒ³ã‚¯IDã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹

    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    # SemanticCoverageã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ã‚’å®Ÿè¡Œ
    from helper_rag_qa import SemanticCoverage

    semantic_analyzer = SemanticCoverage(embedding_model="gemini-embedding-001")

    # æ®µè½å„ªå…ˆã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ã‚’å®Ÿè¡Œ
    # prefer_paragraphs=True: æ®µè½å¢ƒç•Œã‚’æœ€å„ªå…ˆ
    # max_tokens: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    # min_tokens: æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå°ã•ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã¯è‡ªå‹•ãƒãƒ¼ã‚¸ï¼‰
    # verbose=False: è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
    semantic_chunks = semantic_analyzer.create_semantic_chunks(
        document=text,
        max_tokens=max_tokens,
        min_tokens=50,  # æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        prefer_paragraphs=True,  # æ®µè½å„ªå…ˆãƒ¢ãƒ¼ãƒ‰
        verbose=False
    )

    # SemanticCoverageã®å‡ºåŠ›å½¢å¼ã‚’a02ã®å½¢å¼ã«å¤‰æ›
    chunks = []
    tokenizer = tiktoken.get_encoding("cl100k_base")

    for i, semantic_chunk in enumerate(semantic_chunks):
        chunk_text = semantic_chunk['text']
        chunk_tokens = len(tokenizer.encode(chunk_text))

        chunks.append({
            'id': f"{chunk_id_prefix}_{i}",
            'text': chunk_text,
            'tokens': chunk_tokens,
            'type': semantic_chunk.get('type', 'unknown'),  # paragraph/sentence_group/forced_split
            'sentences': semantic_chunk.get('sentences', [])
        })

    return chunks


# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
# ==========================================

def load_uploaded_file(file_path: str) -> pd.DataFrame:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆa00_rag.pyã¨åŒç­‰ï¼‰

    CSV/TXT/JSON/JSONLå½¢å¼ã«å¯¾å¿œã—ã€Combined_Textã‚«ãƒ©ãƒ ã‚’è‡ªå‹•ç”Ÿæˆ

    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        Combined_Textã‚«ãƒ©ãƒ ã‚’å«ã‚€DataFrame
    """
    from helper_rag import clean_text

    file_path_obj = Path(file_path)
    if not file_path_obj.exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    file_extension = file_path_obj.suffix.lower().lstrip('.')

    logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {file_path} (å½¢å¼: {file_extension})")

    try:
        if file_extension == 'csv':
            # CSVãƒ•ã‚¡ã‚¤ãƒ«
            df = pd.read_csv(file_path)

        elif file_extension in ['txt', 'text']:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            df = pd.DataFrame({'text': lines})

        elif file_extension == 'json':
            # JSONãƒ•ã‚¡ã‚¤ãƒ«
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if isinstance(data, list):
                df = pd.DataFrame(data)
            elif isinstance(data, dict):
                df = pd.DataFrame([data])
            else:
                raise ValueError("JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒªã‚¹ãƒˆã¾ãŸã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")

        elif file_extension == 'jsonl':
            # JSON Linesãƒ•ã‚¡ã‚¤ãƒ«
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = [json.loads(line) for line in f if line.strip()]
            df = pd.DataFrame(lines)

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_extension}")

        logger.info(f"  âœ… {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

        # Combined_Textã‚«ãƒ©ãƒ ã®ä½œæˆ
        if 'Combined_Text' not in df.columns:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¢ã™
            text_candidates = ['text', 'content', 'body', 'document', 'answer', 'question']
            found_field = None

            for field in text_candidates:
                if field in df.columns:
                    found_field = field
                    break

            if found_field:
                df['Combined_Text'] = df[found_field].apply(
                    lambda x: clean_text(str(x)) if x is not None else ""
                )
                logger.info(f"  âœ… '{found_field}' ã‚«ãƒ©ãƒ ã‹ã‚‰Combined_Textã‚’ç”Ÿæˆ")
            else:
                # å…¨ã‚«ãƒ©ãƒ ã‚’çµåˆ
                df['Combined_Text'] = df.apply(
                    lambda row: " ".join([str(v) for v in row.values if v is not None]),
                    axis=1
                )
                logger.info("  âœ… å…¨ã‚«ãƒ©ãƒ ã‚’çµåˆã—ã¦Combined_Textã‚’ç”Ÿæˆ")

        # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
        before_len = len(df)
        df = df[df['Combined_Text'].str.strip() != '']
        removed = before_len - len(df)
        if removed > 0:
            logger.info(f"  ğŸ“Š ç©ºãƒ‡ãƒ¼ã‚¿é™¤å¤–: {removed} ä»¶ã‚’é™¤å¤–ï¼ˆæ®‹ã‚Š {len(df)} ä»¶ï¼‰")

        df = df.reset_index(drop=True)
        logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")

        return df

    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise


def load_local_qa_file(file_path: str) -> pd.DataFrame:
    """ãƒ­ãƒ¼ã‚«ãƒ«ã®Q/A CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰

    Args:
        file_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        question, answerã‚«ãƒ©ãƒ ã‚’å«ã‚€DataFrame
    """
    if not Path(file_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«Q/Aãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    df = pd.read_csv(file_path)

    # question, answerã‚«ãƒ©ãƒ ã‚’æ¢ã™
    question_col = None
    answer_col = None

    for col in df.columns:
        col_lower = col.lower()
        if 'question' in col_lower and not question_col:
            question_col = col
        if 'answer' in col_lower and not answer_col:
            answer_col = col

    # question, answerã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if not question_col or not answer_col:
        raise ValueError(f"question ã¾ãŸã¯ answer ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ©ãƒ : {df.columns.tolist()}")

    logger.info(f"  âœ… questionã‚«ãƒ©ãƒ : {question_col}")
    logger.info(f"  âœ… answerã‚«ãƒ©ãƒ : {answer_col}")

    # question, answerã®ã¿æŠ½å‡º
    df_qa = df[[question_col, answer_col]].copy()
    df_qa.columns = ['question', 'answer']  # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€

    # ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
    before_len = len(df_qa)
    df_qa = df_qa.dropna(subset=['question', 'answer'])
    df_qa = df_qa[
        (df_qa['question'].str.strip() != '') &
        (df_qa['answer'].str.strip() != '')
    ]
    removed = before_len - len(df_qa)
    if removed > 0:
        logger.info(f"ğŸ“Š ç©ºãƒ‡ãƒ¼ã‚¿é™¤å¤–: {removed} ä»¶ã‚’é™¤å¤–ï¼ˆæ®‹ã‚Š {len(df_qa)} ä»¶ï¼‰")

    # é‡è¤‡é™¤å»
    before_len = len(df_qa)
    df_qa = df_qa.drop_duplicates()
    removed = before_len - len(df_qa)
    if removed > 0:
        logger.info(f"ğŸ“Š é‡è¤‡é™¤å»: {removed} ä»¶ã‚’é™¤å¤–ï¼ˆæ®‹ã‚Š {len(df_qa)} ä»¶ï¼‰")

    df_qa = df_qa.reset_index(drop=True)
    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df_qa)}ä»¶ã®Q/Aãƒšã‚¢")

    return df_qa


def load_preprocessed_data(dataset_type: str) -> pd.DataFrame:
    """preprocessedãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    Args:
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
    Returns:
        èª­ã¿è¾¼ã‚“ã DataFrame
    """
    config = DATASET_CONFIGS.get(dataset_type)
    if not config:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    file_path = config["file"]
    file_path_obj = Path(file_path)

    # å›ºå®šåã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if not file_path_obj.exists():
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        # ä¾‹: preprocessed_wikipedia_ja.csv â†’ preprocessed_wikipedia_ja_*.csv ã‚’æ¤œç´¢
        base_name = file_path_obj.stem  # preprocessed_wikipedia_ja
        extension = file_path_obj.suffix  # .csv
        pattern = f"{base_name}_*{extension}"

        output_dir = file_path_obj.parent
        matching_files = list(output_dir.glob(pattern))

        if not matching_files:
            raise FileNotFoundError(
                f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}\n"
                f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{output_dir}/{pattern}ï¼‰ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚½ãƒ¼ãƒˆï¼‰
        # ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼: preprocessed_wikipedia_ja_20251125_045518.csv
        matching_files.sort(key=lambda x: x.name)
        file_path = str(matching_files[-1])
        logger.info(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•é¸æŠ: {Path(file_path).name}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    df = pd.read_csv(file_path)

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    text_col = config["text_column"]
    if text_col not in df.columns:
        raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df = df[df[text_col].notna() & (df[text_col].str.strip() != '')]

    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    return df


def create_document_chunks(df: pd.DataFrame, dataset_type: str, max_docs: Optional[int] = None, config: Optional[Dict] = None) -> List[Dict]:
    """DataFrameã‹ã‚‰æ–‡æ›¸ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ï¼‰
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        max_docs: å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆæŒ‡å®šãŒãªã„å ´åˆã¯DATASET_CONFIGSã‹ã‚‰å–å¾—ï¼‰
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    if config is None:
        config = DATASET_CONFIGS.get(dataset_type)
        if not config:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    text_col = config["text_column"]
    title_col = config.get("title_column")
    chunk_size = config["chunk_size"]
    lang = config["lang"]

    all_chunks = []

    # å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°ã‚’åˆ¶é™
    docs_to_process = df.head(max_docs) if max_docs else df

    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆé–‹å§‹: {len(docs_to_process)}ä»¶ã®æ–‡æ›¸ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ï¼‰")

    total_docs = len(docs_to_process)
    for i, (idx, row) in enumerate(docs_to_process.iterrows()):
        # é€²æ—ãƒ­ã‚°ï¼ˆ10ä»¶ã”ã¨ï¼‰
        if (i + 1) % 10 == 0 or (i + 1) == total_docs:
            logger.info(f"  ãƒãƒ£ãƒ³ã‚¯ä½œæˆé€²æ—: {i + 1}/{total_docs} æ–‡æ›¸å®Œäº†")

        # row[text_col]ã¯Seriesã‚„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ˜ç¤ºçš„ã«strã«å¤‰æ›
        text = str(row[text_col]) if pd.notna(row[text_col]) else ""

        # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
        if title_col and title_col in row and pd.notna(row[title_col]):
            doc_id = f"{dataset_type}_{idx}_{str(row[title_col])[:30]}"
        else:
            doc_id = f"{dataset_type}_{idx}"

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ã«ã‚ˆã‚‹ãƒãƒ£ãƒ³ã‚¯ä½œæˆã‚’ä½¿ç”¨
        try:
            chunk_id_prefix = f"{doc_id}_chunk"
            chunks = create_semantic_chunks(
                text=text,
                lang=lang,
                max_tokens=chunk_size,
                chunk_id_prefix=chunk_id_prefix
            )

            # å„ãƒãƒ£ãƒ³ã‚¯ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            for i, chunk in enumerate(chunks):
                chunk['doc_id'] = doc_id
                chunk['doc_idx'] = idx
                chunk['chunk_idx'] = i
                chunk['dataset_type'] = dataset_type
                all_chunks.append(chunk)

        except Exception as e:
            logger.warning(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆã‚¨ãƒ©ãƒ¼ (doc {idx}): {e}")
            continue

    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆå®Œäº†: {len(all_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²ï¼‰")
    return all_chunks


def merge_small_chunks(chunks: List[Dict], min_tokens: int = 150, max_tokens: int = 400) -> List[Dict]:
    """å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã‚’çµ±åˆã—ã¦é©åˆ‡ãªã‚µã‚¤ã‚ºã«ã™ã‚‹
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        min_tokens: ã“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°æœªæº€ã®ãƒãƒ£ãƒ³ã‚¯ã¯çµ±åˆå¯¾è±¡
        max_tokens: çµ±åˆå¾Œã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    Returns:
        çµ±åˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")
    merged_chunks = []
    current_merge = None

    for chunk in chunks:
        chunk_tokens = len(tokenizer.encode(chunk['text']))

        # å¤§ãã„ãƒãƒ£ãƒ³ã‚¯ã¯ãã®ã¾ã¾è¿½åŠ 
        if chunk_tokens >= min_tokens:
            if current_merge:
                merged_chunks.append(current_merge)
                current_merge = None
            merged_chunks.append(chunk)
        else:
            # å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã¯çµ±åˆå€™è£œ
            if current_merge is None:
                current_merge = chunk.copy()
                current_merge['merged'] = True
                current_merge['original_chunks'] = [chunk['id']]
            else:
                # çµ±åˆå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                merge_tokens = len(tokenizer.encode(current_merge['text']))
                if merge_tokens + chunk_tokens <= max_tokens:
                    # åŒã˜æ–‡æ›¸ã‹ã‚‰ã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿çµ±åˆ
                    if current_merge.get('doc_id') == chunk.get('doc_id'):
                        current_merge['text'] += "\n\n" + chunk['text']
                        current_merge['original_chunks'].append(chunk['id'])
                        if 'chunk_idx' in current_merge:
                            current_merge['chunk_idx'] = f"{current_merge['chunk_idx']}-{chunk['chunk_idx']}"
                    else:
                        # ç•°ãªã‚‹æ–‡æ›¸ã®å ´åˆã¯åˆ¥ã€…ã«
                        merged_chunks.append(current_merge)
                        current_merge = chunk.copy()
                        current_merge['merged'] = True
                        current_merge['original_chunks'] = [chunk['id']]
                else:
                    # ã‚µã‚¤ã‚ºã‚ªãƒ¼ãƒãƒ¼ã®å ´åˆã¯ç¾åœ¨ã®çµ±åˆã‚’è¿½åŠ ã—ã¦æ–°è¦é–‹å§‹
                    merged_chunks.append(current_merge)
                    current_merge = chunk.copy()
                    current_merge['merged'] = True
                    current_merge['original_chunks'] = [chunk['id']]

    # æœ€å¾Œã®çµ±åˆãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
    if current_merge:
        merged_chunks.append(current_merge)

    logger.info(f"ãƒãƒ£ãƒ³ã‚¯çµ±åˆ: {len(chunks)}å€‹ â†’ {len(merged_chunks)}å€‹ ({100*(1-len(merged_chunks)/len(chunks)):.1f}%å‰Šæ¸›)")
    return merged_chunks


# ==========================================
# Q/Aãƒšã‚¢ç”Ÿæˆ
# ==========================================

def determine_qa_count(chunk: Dict, config: Dict) -> int:
    """ãƒãƒ£ãƒ³ã‚¯ã«æœ€é©ãªQ/Aæ•°ã‚’æ±ºå®šï¼ˆæ”¹å–„ç‰ˆï¼šå‹•çš„èª¿æ•´ï¼‰
    Args:
        chunk: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    Returns:
        Q/Aãƒšã‚¢æ•°
    """
    base_count = config["qa_per_chunk"]
    client_for_token_count = create_llm_client(provider="gemini", default_model=model) # modelã¯LLMç”Ÿæˆãƒ¢ãƒ‡ãƒ«
    token_count = client_for_token_count.count_tokens(chunk['text'], model=model)

    # ãƒãƒ£ãƒ³ã‚¯ä½ç½®ã‚’è€ƒæ…®ï¼ˆæ–‡æ›¸å¾ŒåŠã®è£œæ­£ï¼‰
    chunk_position = chunk.get('chunk_idx', 0)

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ãåŸºæœ¬Q&Aæ•°æ±ºå®šï¼ˆæ”¹å–„ç‰ˆï¼‰
    if token_count < 50:
        qa_count = 2  # æ—§: 1 â†’ æ–°: 2ï¼ˆçŸ­ã„ãƒãƒ£ãƒ³ã‚¯ã§ã‚‚æœ€ä½2å€‹ï¼‰
    elif token_count < 100:
        qa_count = 3  # æ—§: 2 â†’ æ–°: 3ï¼ˆShortãƒãƒ£ãƒ³ã‚¯å¼·åŒ–ï¼‰
    elif token_count < 200:
        qa_count = base_count + 1  # Mediumã¯+1
    elif token_count < 300:
        qa_count = base_count + 2  # Longãƒãƒ£ãƒ³ã‚¯ã¯+2
    else:
        qa_count = base_count + 3  # è¶…é•·æ–‡ã¯+3

    # æ–‡æ›¸å¾ŒåŠã®ä½ç½®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ï¼ˆ6ç•ªç›®ä»¥é™ã®ãƒãƒ£ãƒ³ã‚¯ã¯+1ï¼‰
    if isinstance(chunk_position, int) and chunk_position >= 5:
        qa_count += 1

    return min(qa_count, 8)  # ä¸Šé™ã‚’8ã«è¨­å®š


def generate_qa_pairs_for_batch(
    chunks: List[Dict],
    config: Dict,
    model: str = "gemini-2.0-flash",
    client: Optional[LLMClient] = None
) -> List[Dict]:
    """è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ä¸€åº¦ã«Q/Aãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼‰
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆ1-5å€‹ï¼‰
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰
        client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆGeminiClientï¼‰
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    if client is None:
        client = create_llm_client(provider="gemini")

    if len(chunks) == 0:
        return []

    # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã¯å¾“æ¥ã®å‡¦ç†
    if len(chunks) == 1:
        return generate_qa_pairs_for_chunk(chunks[0], config, model, client)

    lang = config["lang"]
    all_qa_pairs = []

    # è¨€èªåˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    if lang == "ja":
        system_prompt = """ã‚ãªãŸã¯æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚
è¤‡æ•°ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€å­¦ç¿’åŠ¹æœã®é«˜ã„Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ç”Ÿæˆãƒ«ãƒ¼ãƒ«:
1. è³ªå•ã¯æ˜ç¢ºã§å…·ä½“çš„ã«
2. å›ç­”ã¯ç°¡æ½”ã§æ­£ç¢ºã«ï¼ˆ1-2æ–‡ç¨‹åº¦ï¼‰
3. ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«å¿ å®Ÿã«
4. å¤šæ§˜ãªè¦³ç‚¹ã‹ã‚‰è³ªå•ã‚’ä½œæˆ"""

        # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        combined_text = ""
        chunks_data = {}
        total_pairs = 0

        for i, chunk in enumerate(chunks, 1):
            num_pairs = determine_qa_count(chunk, config)
            total_pairs += num_pairs
            chunk_text = chunk['text']

            # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
            if len(chunk_text) > 1000:
                chunk_text = chunk_text[:1000] + "..."

            combined_text += f"\n\nã€ãƒ†ã‚­ã‚¹ãƒˆ{i}ã€‘\n{chunk_text}"
            chunks_data[f"chunk_{i}"] = {"num_pairs": num_pairs, "chunk": chunk}

        user_prompt = f"""ä»¥ä¸‹ã®{len(chunks)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€åˆè¨ˆ{total_pairs}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
{combined_text}

è³ªå•ã‚¿ã‚¤ãƒ—:
- fact: äº‹å®Ÿç¢ºèªå‹ï¼ˆã€œã¯ä½•ã§ã™ã‹ï¼Ÿï¼‰
- reason: ç†ç”±èª¬æ˜å‹ï¼ˆãªãœã€œã§ã™ã‹ï¼Ÿï¼‰
- comparison: æ¯”è¼ƒå‹ï¼ˆã€œã¨ã€œã®é•ã„ã¯ï¼Ÿï¼‰
- application: å¿œç”¨å‹ï¼ˆã€œã¯ã©ã®ã‚ˆã†ã«æ´»ç”¨ã•ã‚Œã¾ã™ã‹ï¼Ÿï¼‰

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "qa_pairs": [
    {{
      "question": "è³ªå•æ–‡",
      "answer": "å›ç­”æ–‡",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

    else:
        system_prompt = """You are an expert in educational content creation.
Generate high-quality Q&A pairs from multiple English texts.

Generation rules:
1. Questions should be clear and specific
2. Answers should be concise and accurate (1-2 sentences)
3. Stay faithful to the text content
4. Create questions from diverse perspectives"""

        # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        combined_text = ""
        chunks_data = {}
        total_pairs = 0

        for i, chunk in enumerate(chunks, 1):
            num_pairs = determine_qa_count(chunk, config)
            total_pairs += num_pairs
            chunk_text = chunk['text']

            # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
            if len(chunk_text) > 1000:
                chunk_text = chunk_text[:1000] + "..."

            combined_text += f"\n\nã€Text {i}ã€‘\n{chunk_text}"
            chunks_data[f"chunk_{i}"] = {"num_pairs": num_pairs, "chunk": chunk}

        user_prompt = f"""Generate {total_pairs} Q&A pairs from the following {len(chunks)} texts.
{combined_text}

Question types:
- fact: Factual questions (What is...?)
- reason: Explanatory questions (Why...?)
- comparison: Comparative questions (What's the difference...?)
- application: Application questions (How is... used?)

Output in JSON format:
{{
  "qa_pairs": [
    {{
      "question": "question text",
      "answer": "answer text",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

    try:
        # Gemini API ã‚’ä½¿ç”¨ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›ï¼‰
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ±åˆ
        combined_input = f"{system_prompt}\n\n{user_prompt}"

        # GeminiClientã®æ§‹é€ åŒ–å‡ºåŠ›ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        parsed_data = client.generate_structured(
            prompt=combined_input,
            response_schema=QAPairsResponse,
            model=model,
            max_output_tokens=4000  # ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚å¢—åŠ ï¼ˆ3ãƒãƒ£ãƒ³ã‚¯å¯¾å¿œï¼‰
        )

        # ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã‚’å„ãƒãƒ£ãƒ³ã‚¯ã«åˆ†é…
        # å„ãƒãƒ£ãƒ³ã‚¯ã«æœŸå¾…ã•ã‚Œã‚‹æ•°ã ã‘Q/Aã‚’å‰²ã‚Šå½“ã¦
        qa_index = 0
        for i, chunk in enumerate(chunks, 1):
            chunk_key = f"chunk_{i}"
            expected_pairs = chunks_data[chunk_key]["num_pairs"]

            # ã“ã®ãƒãƒ£ãƒ³ã‚¯ã«å‰²ã‚Šå½“ã¦ã‚‹Q/Aãƒšã‚¢ã‚’å–å¾—
            for _ in range(expected_pairs):
                if qa_index < len(parsed_data.qa_pairs):
                    qa_data = parsed_data.qa_pairs[qa_index]
                    qa = {
                        "question": qa_data.question,
                        "answer": qa_data.answer,
                        "question_type": qa_data.question_type,
                        "source_chunk_id": chunk.get('id', ''),
                        "doc_id": chunk.get('doc_id', ''),
                        "dataset_type": chunk.get('dataset_type', ''),
                        "chunk_idx": chunk.get('chunk_idx', 0)
                    }
                    all_qa_pairs.append(qa)
                    qa_index += 1

        # ç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
        if len(all_qa_pairs) == 0:
            logger.error("Gemini APIã‹ã‚‰è§£æå¯èƒ½ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            raise ValueError("No parseable response from Gemini API")

        return all_qa_pairs

    except Exception as e:
        logger.error(f"ãƒãƒƒãƒQ/Aç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.debug(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥å‡¦ç†
        logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ£ãƒ³ã‚¯ã‚’å€‹åˆ¥å‡¦ç†ã—ã¾ã™")
        for chunk in chunks:
            try:
                qa_pairs = generate_qa_pairs_for_chunk(chunk, config, model, client)
                all_qa_pairs.extend(qa_pairs)
            except Exception as chunk_error:
                logger.error(f"ãƒãƒ£ãƒ³ã‚¯å€‹åˆ¥å‡¦ç†ã‚¨ãƒ©ãƒ¼: {chunk_error}")
        return all_qa_pairs


def generate_qa_pairs_for_chunk(
    chunk: Dict,
    config: Dict,
    model: str = "gemini-2.0-flash",
    client: Optional[LLMClient] = None
) -> List[Dict]:
    """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆ
    Args:
        chunk: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰
        client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆGeminiClientï¼‰
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    if client is None:
        client = create_llm_client(provider="gemini")

    num_pairs = determine_qa_count(chunk, config)
    lang = config["lang"]

    # è¨€èªåˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    if lang == "ja":
        system_prompt = """ã‚ãªãŸã¯æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€å­¦ç¿’åŠ¹æœã®é«˜ã„Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ç”Ÿæˆãƒ«ãƒ¼ãƒ«:
1. è³ªå•ã¯æ˜ç¢ºã§å…·ä½“çš„ã«
2. å›ç­”ã¯ç°¡æ½”ã§æ­£ç¢ºã«ï¼ˆ1-2æ–‡ç¨‹åº¦ï¼‰
3. ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«å¿ å®Ÿã«
4. å¤šæ§˜ãªè¦³ç‚¹ã‹ã‚‰è³ªå•ã‚’ä½œæˆ"""

        question_types_desc = """
- fact: äº‹å®Ÿç¢ºèªå‹ï¼ˆã€œã¯ä½•ã§ã™ã‹ï¼Ÿï¼‰
- reason: ç†ç”±èª¬æ˜å‹ï¼ˆãªãœã€œã§ã™ã‹ï¼Ÿï¼‰
- comparison: æ¯”è¼ƒå‹ï¼ˆã€œã¨ã€œã®é•ã„ã¯ï¼Ÿï¼‰
- application: å¿œç”¨å‹ï¼ˆã€œã¯ã©ã®ã‚ˆã†ã«æ´»ç”¨ã•ã‚Œã¾ã™ã‹ï¼Ÿï¼‰"""
    else:
        system_prompt = """You are an expert in educational content creation.
Generate high-quality Q&A pairs from the given English text.

Generation rules:
1. Questions should be clear and specific
2. Answers should be concise and accurate (1-2 sentences)
3. Stay faithful to the text content
4. Create questions from diverse perspectives"""

        question_types_desc = """
- fact: Factual questions (What is...?)
- reason: Explanatory questions (Why...?)
- comparison: Comparative questions (What's the difference...?)
- application: Application questions (How is... used?)"""

    # è¨€èªã«å¿œã˜ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    if lang == "ja":
        user_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰{num_pairs}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è³ªå•ã‚¿ã‚¤ãƒ—:
{question_types_desc}

ãƒ†ã‚­ã‚¹ãƒˆ:
{chunk['text']}

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "qa_pairs": [
    {{
      "question": "è³ªå•æ–‡",
      "answer": "å›ç­”æ–‡",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""
    else:
        user_prompt = f"""Generate {num_pairs} Q&A pairs from the following text.

Question types:
{question_types_desc}

Text:
{chunk['text']}

Output in JSON format:
{{
  "qa_pairs": [
    {{
      "question": "question text",
      "answer": "answer text",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

    try:
        # ãƒãƒ£ãƒ³ã‚¯ãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®ï¼ˆæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã„å‚¾å‘ãŒã‚ã‚‹ãŸã‚ï¼‰
        max_chunk_length = 2000  # æ–‡å­—æ•°åˆ¶é™
        chunk_text = chunk['text']
        if len(chunk_text) > max_chunk_length:
            chunk_text = chunk_text[:max_chunk_length] + "..."
            logger.debug(f"ãƒãƒ£ãƒ³ã‚¯ã‚’{max_chunk_length}æ–‡å­—ã«çŸ­ç¸®")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†æ§‹ç¯‰ï¼ˆçŸ­ç¸®ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰
        if lang == "ja":
            user_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰{num_pairs}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è³ªå•ã‚¿ã‚¤ãƒ—:
{question_types_desc}

ãƒ†ã‚­ã‚¹ãƒˆ:
{chunk_text}

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "qa_pairs": [
    {{
      "question": "è³ªå•æ–‡",
      "answer": "å›ç­”æ–‡",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""
        else:
            user_prompt = f"""Generate {num_pairs} Q&A pairs from the following text.

Question types:
{question_types_desc}

Text:
{chunk_text}

Output in JSON format:
{{
  "qa_pairs": [
    {{
      "question": "question text",
      "answer": "answer text",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

        # Gemini API ã‚’ä½¿ç”¨ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›ï¼‰
        combined_input = f"{system_prompt}\n\n{user_prompt}"

        # GeminiClientã®æ§‹é€ åŒ–å‡ºåŠ›ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        parsed_data = client.generate_structured(
            prompt=combined_input,
            response_schema=QAPairsResponse,
            model=model,
            max_output_tokens=1000
        )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ
        qa_pairs = []
        for qa_data in parsed_data.qa_pairs:
            qa = {
                "question": qa_data.question,
                "answer": qa_data.answer,
                "question_type": qa_data.question_type,
                "source_chunk_id": chunk.get('id', ''),
                "doc_id": chunk.get('doc_id', ''),
                "dataset_type": chunk.get('dataset_type', ''),
                "chunk_idx": chunk.get('chunk_idx', 0)
            }
            qa_pairs.append(qa)

        # ç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
        if len(qa_pairs) == 0:
            logger.error(f"Gemini APIã‹ã‚‰è§£æå¯èƒ½ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ (chunk {chunk.get('id', 'unknown')})")
            raise ValueError("No parseable response from Gemini API")

        return qa_pairs

    except Exception as e:
        logger.error(f"Q/Aç”Ÿæˆã‚¨ãƒ©ãƒ¼ (chunk {chunk.get('id', 'unknown')}): {e}")
        import traceback
        logger.debug(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
        return []


def generate_qa_for_dataset(
    chunks: List[Dict],
    dataset_type: str,
    model: str = "gemini-2.0-flash",
    chunk_batch_size: int = 3,
    merge_chunks: bool = True,
    min_tokens: int = 150,
    max_tokens: int = 400,
    config: Optional[Dict] = None
) -> List[Dict]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®Q/Aãƒšã‚¢ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼‰
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰
        chunk_batch_size: 1å›ã®APIã§å‡¦ç†ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆ1-5ï¼‰
        merge_chunks: å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã‚’çµ±åˆã™ã‚‹ã‹
        min_tokens: çµ±åˆå¯¾è±¡ã®æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        max_tokens: çµ±åˆå¾Œã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆæŒ‡å®šãŒãªã„å ´åˆã¯DATASET_CONFIGSã‹ã‚‰å–å¾—ï¼‰
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    if config is None:
        config = DATASET_CONFIGS.get(dataset_type)
        if not config:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    client = create_llm_client(provider="gemini")
    all_qa_pairs = []

    # ãƒãƒ£ãƒ³ã‚¯ã®å‰å‡¦ç†ï¼ˆå°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã®çµ±åˆï¼‰
    if merge_chunks:
        processed_chunks = merge_small_chunks(chunks, min_tokens, max_tokens)
    else:
        processed_chunks = chunks

    total_chunks = len(processed_chunks)
    api_calls = (total_chunks + chunk_batch_size - 1) // chunk_batch_size

    logger.info(f"""
    Q/Aãƒšã‚¢ç”Ÿæˆé–‹å§‹:
    - å…ƒãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}
    - å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {total_chunks}
    - ãƒãƒƒãƒã‚µã‚¤ã‚º: {chunk_batch_size}
    - APIå‘¼ã³å‡ºã—äºˆå®š: {api_calls}å›
    - ãƒ¢ãƒ‡ãƒ«: {model}
    """)

    # ãƒãƒƒãƒå‡¦ç†
    for i in range(0, total_chunks, chunk_batch_size):
        batch = processed_chunks[i:i+chunk_batch_size]
        batch_num = i // chunk_batch_size + 1
        total_batches = api_calls

        logger.info(f"ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†ä¸­ ({len(batch)}ãƒãƒ£ãƒ³ã‚¯)...")

        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãQ/Aç”Ÿæˆ
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if chunk_batch_size == 1:
                    # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
                    qa_pairs = generate_qa_pairs_for_chunk(batch[0], config, model, client)
                else:
                    # ãƒãƒƒãƒå‡¦ç†
                    qa_pairs = generate_qa_pairs_for_batch(batch, config, model, client)

                if qa_pairs:
                    all_qa_pairs.extend(qa_pairs)
                    logger.debug(f"ãƒãƒƒãƒ {batch_num}: {len(qa_pairs)}å€‹ã®Q/Aãƒšã‚¢ç”Ÿæˆ")
                break

            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"ãƒãƒƒãƒ {batch_num} ç”Ÿæˆå¤±æ•—: {e}")
                    # æœ€çµ‚è©¦è¡Œå¤±æ•—æ™‚ã¯å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    logger.info("å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    for chunk in batch:
                        try:
                            qa_pairs = generate_qa_pairs_for_chunk(chunk, config, model, client)
                            if qa_pairs:
                                all_qa_pairs.extend(qa_pairs)
                        except Exception as chunk_error:
                            logger.error(f"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {chunk_error}")
                else:
                    wait_time = 2 ** attempt
                    logger.warning(f"ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries} (å¾…æ©Ÿ: {wait_time}ç§’)")
                    time.sleep(wait_time)

        # APIåˆ¶é™å¯¾ç­–ï¼ˆæœ€å¾Œã®ãƒãƒƒãƒä»¥å¤–ã§å¾…æ©Ÿï¼‰
        if i + chunk_batch_size < total_chunks:
            time.sleep(0.2)  # çŸ­ç¸®ï¼ˆãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šå‘¼ã³å‡ºã—æ•°ãŒæ¸›ã£ã¦ã„ã‚‹ãŸã‚ï¼‰

    logger.info(f"""
    Q/Aãƒšã‚¢ç”Ÿæˆå®Œäº†:
    - ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢: {len(all_qa_pairs)}å€‹
    - å®Ÿè¡Œã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—: ç´„{api_calls}å›
    """)

    return all_qa_pairs


# ==========================================
# Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†
# ==========================================

def check_celery_workers(required_workers: int = 8) -> bool:
    """Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
    Args:
        required_workers: å¿…è¦ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    Returns:
        ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæ­£å¸¸ã«ç¨¼åƒã—ã¦ã„ã‚‹å ´åˆTrue
    """
    try:
        logger.info("Celeryè¨­å®šã‚’èª­ã¿è¾¼ã¿ä¸­...")
        from celery_tasks import app as celery_app
        logger.info("Celeryè¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")

        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã®statsæƒ…å ±ã‚’å–å¾—ï¼ˆæœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
        inspect = celery_app.control.inspect(timeout=2.0)
        stats = None

        logger.info("ãƒ¯ãƒ¼ã‚«ãƒ¼çŠ¶æ…‹ã‚’å•ã„åˆã‚ã›ä¸­...")
        for attempt in range(3):
            stats = inspect.stats()
            if stats:
                break
            if attempt < 2:
                logger.debug(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ç¢ºèªãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/3...")
                time.sleep(1)

        if not stats:
            logger.warning("âš ï¸  Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ï¼ˆå¿œç­”ãªã—ï¼‰")
            logger.info("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
            logger.info(f"  ./start_celery.sh start -w {required_workers}")
            logger.info("\nã¾ãŸã¯:")
            logger.info("  redis-cli FLUSHDB")
            logger.info(f"  ./start_celery.sh restart -w {required_workers}")
            return False

        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        worker_count = 0
        for worker_name, worker_stats in stats.items():
            # å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ãƒ—ãƒ¼ãƒ«æƒ…å ±ã‹ã‚‰å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å–å¾—
            pool_size = worker_stats.get('pool', {}).get('max-concurrency', 1)
            worker_count += pool_size

        if worker_count == 0:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¯ãƒ¼ã‚«ãƒ¼åã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            worker_count = len(stats)

        if worker_count < required_workers:
            logger.warning(f"âš ï¸  ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ä¸è¶³: {worker_count}/{required_workers}å€‹ç¨¼åƒä¸­")
            logger.info(f"æ¨å¥¨: ./start_celery.sh restart -w {required_workers}")
            # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ãŒå°‘ãªãã¦ã‚‚ç¶šè¡Œå¯èƒ½
            return True

        logger.info(f"âœ“ Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ç¢ºèªå®Œäº†: {worker_count}å€‹ç¨¼åƒä¸­")
        return True

    except ImportError as e:
        logger.error(f"celery_tasks ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return False
    except Exception as e:
        logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        logger.info("Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ãŒèµ·å‹•ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        logger.info(f"èµ·å‹•ã‚³ãƒãƒ³ãƒ‰: ./start_celery.sh start -w {required_workers}")
        return False


# ==========================================
# ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
# ==========================================

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤è¨­å®š
OPTIMAL_THRESHOLDS = {
    "cc_news": {
        "strict": 0.80,
        "standard": 0.70,
        "lenient": 0.60
    },
    "japanese_text": {
        "strict": 0.75,
        "standard": 0.65,
        "lenient": 0.55
    },
    "wikipedia_ja": {
        "strict": 0.85,   # å°‚é–€çš„ãªå†…å®¹ â†’ é«˜ã„é¡ä¼¼åº¦è¦æ±‚
        "standard": 0.75,
        "lenient": 0.65
    },
    "livedoor": {
        "strict": 0.78,   # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã¯å…·ä½“çš„ã§å³å¯†æ€§ãŒä¸­ç¨‹åº¦
        "standard": 0.68, # cc_newsã¨wikipedia_jaã®ä¸­é–“
        "lenient": 0.58
    }
}


def get_optimal_thresholds(dataset_type: str) -> Dict[str, float]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®æœ€é©é–¾å€¤ã‚’å–å¾—
    Args:
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
    Returns:
        é–¾å€¤è¾æ›¸ {strict, standard, lenient}
    """
    return OPTIMAL_THRESHOLDS.get(dataset_type, {
        "strict": 0.8,
        "standard": 0.7,
        "lenient": 0.6
    })


def multi_threshold_coverage(coverage_matrix: np.ndarray, chunks: List[Dict],
                             qa_pairs: List[Dict], thresholds: Dict[str, float]) -> Dict:
    """è¤‡æ•°é–¾å€¤ã§ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’è©•ä¾¡
    Args:
        coverage_matrix: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        thresholds: é–¾å€¤è¾æ›¸
    Returns:
        å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸çµæœ
    """
    results = {}
    max_similarities = coverage_matrix.max(axis=1)

    for level, threshold in thresholds.items():
        covered = sum(1 for s in max_similarities if s >= threshold)
        uncovered_chunks = [
            {
                "chunk_id": chunks[i].get("id", f"chunk_{i}"),
                "similarity": float(max_similarities[i]),
                "gap": float(threshold - max_similarities[i])
            }
            for i, sim in enumerate(max_similarities)
            if sim < threshold
        ]

        results[level] = {
            "threshold": threshold,
            "covered_chunks": covered,
            "coverage_rate": covered / len(chunks) if chunks else 0,
            "uncovered_count": len(uncovered_chunks),
            "uncovered_chunks": uncovered_chunks
        }

    return results


def analyze_chunk_characteristics_coverage(chunks: List[Dict], coverage_matrix: np.ndarray,
                                          qa_pairs: List[Dict], threshold: float = 0.7) -> Dict:
    """ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        coverage_matrix: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        threshold: åˆ¤å®šé–¾å€¤
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸çµæœ
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")
    results = {
        "by_length": {},      # é•·ã•åˆ¥
        "by_position": {},    # ä½ç½®åˆ¥
        "summary": {}
    }

    # 1. é•·ã•åˆ¥åˆ†æ
    for i, chunk in enumerate(chunks):
        token_count = len(tokenizer.encode(chunk['text']))
        length_category = (
            "short" if token_count < 100 else
            "medium" if token_count < 200 else
            "long"
        )

        if length_category not in results["by_length"]:
            results["by_length"][length_category] = {
                "count": 0,
                "covered": 0,
                "avg_similarity": 0.0,
                "similarities": []
            }

        max_sim = coverage_matrix[i].max()
        results["by_length"][length_category]["count"] += 1
        results["by_length"][length_category]["similarities"].append(float(max_sim))

        if max_sim >= threshold:
            results["by_length"][length_category]["covered"] += 1

    # å¹³å‡é¡ä¼¼åº¦ã¨ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ã‚’è¨ˆç®—
    for length_cat in results["by_length"]:
        data = results["by_length"][length_cat]
        data["avg_similarity"] = float(np.mean(data["similarities"])) if data["similarities"] else 0.0
        data["coverage_rate"] = data["covered"] / data["count"] if data["count"] > 0 else 0.0
        # similaritiesã¯å¤§ãã„ã®ã§å‰Šé™¤ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        del data["similarities"]

    # 2. ä½ç½®åˆ¥åˆ†æï¼ˆæ–‡æ›¸ã®å‰åŠ/ä¸­ç›¤/å¾ŒåŠï¼‰
    total_chunks = len(chunks)
    for i, chunk in enumerate(chunks):
        position = (
            "beginning" if i < total_chunks * 0.33 else
            "middle" if i < total_chunks * 0.67 else
            "end"
        )

        if position not in results["by_position"]:
            results["by_position"][position] = {
                "count": 0,
                "covered": 0,
                "avg_similarity": 0.0,
                "similarities": []
            }

        max_sim = coverage_matrix[i].max()
        results["by_position"][position]["count"] += 1
        results["by_position"][position]["similarities"].append(float(max_sim))

        if max_sim >= threshold:
            results["by_position"][position]["covered"] += 1

    # å¹³å‡é¡ä¼¼åº¦ã¨ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ã‚’è¨ˆç®—
    for position in results["by_position"]:
        data = results["by_position"][position]
        data["avg_similarity"] = float(np.mean(data["similarities"])) if data["similarities"] else 0.0
        data["coverage_rate"] = data["covered"] / data["count"] if data["count"] > 0 else 0.0
        del data["similarities"]

    # 3. ã‚µãƒãƒªãƒ¼æƒ…å ±
    results["summary"] = {
        "total_chunks": len(chunks),
        "total_qa_pairs": len(qa_pairs),
        "threshold_used": threshold,
        "insights": []
    }

    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
    for length_cat, data in results["by_length"].items():
        if data["coverage_rate"] < 0.7:
            results["summary"]["insights"].append(
                f"{length_cat}ãƒãƒ£ãƒ³ã‚¯ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ{data['coverage_rate']:.1%}ï¼‰"
            )

    for position, data in results["by_position"].items():
        if data["coverage_rate"] < 0.7:
            results["summary"]["insights"].append(
                f"æ–‡æ›¸{position}éƒ¨åˆ†ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ{data['coverage_rate']:.1%}ï¼‰"
            )

    return results


def analyze_coverage(chunks: List[Dict], qa_pairs: List[Dict], dataset_type: str = "wikipedia_ja",
                     custom_threshold: Optional[float] = None) -> Dict:
    """ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’åˆ†æï¼ˆå¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æå¯¾å¿œï¼‰
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆé–¾å€¤è‡ªå‹•è¨­å®šã«ä½¿ç”¨ï¼‰
        custom_threshold: ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤ï¼ˆæŒ‡å®šæ™‚ã¯ã“ã‚Œã‚’ä½¿ç”¨ï¼‰
    Returns:
        ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœï¼ˆå¤šæ®µéšè©•ä¾¡ã€ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ†æã‚’å«ã‚€ï¼‰
    """
    from helper_rag_qa import SemanticCoverage
    analyzer = SemanticCoverage()

    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆãƒãƒƒãƒAPIæœ€é©åŒ–ç‰ˆï¼‰
    logger.info("=" * 60)
    logger.info("ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆé–‹å§‹")
    logger.info("=" * 60)

    # ãƒãƒ£ãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
    logger.info(f"[Step 1/3] ãƒãƒ£ãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {len(chunks)}ä»¶")
    doc_embeddings = analyzer.generate_embeddings(chunks)
    logger.info(f"[Step 1/3] ãƒãƒ£ãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿å®Œäº†: {len(doc_embeddings)}ä»¶")

    # Q&Aãƒšã‚¢ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆãƒãƒƒãƒAPIä½¿ç”¨ã§é«˜é€ŸåŒ–ï¼‰
    qa_texts = [f"{qa['question']} {qa['answer']}" for qa in qa_pairs]
    logger.info(f"[Step 2/3] Q/Aãƒšã‚¢åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {len(qa_texts)}ä»¶")
    qa_embeddings = analyzer.generate_embeddings_batch(qa_texts, batch_size=2048)
    logger.info(f"[Step 2/3] Q/Aãƒšã‚¢åŸ‹ã‚è¾¼ã¿å®Œäº†: {len(qa_embeddings)}ä»¶")

    if len(qa_embeddings) == 0:
        return {
            "coverage_rate": 0.0,
            "covered_chunks": 0,
            "total_chunks": len(chunks),
            "uncovered_chunks": chunks,
            "multi_threshold": {},
            "chunk_analysis": {}
        }

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—è¨ˆç®—
    logger.info("ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—è¨ˆç®—ä¸­...")
    coverage_matrix = np.zeros((len(chunks), len(qa_pairs)))
    for i in range(len(doc_embeddings)):
        for j in range(len(qa_embeddings)):
            similarity = analyzer.cosine_similarity(doc_embeddings[i], qa_embeddings[j])
            coverage_matrix[i, j] = similarity

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤ã‚’å–å¾—
    thresholds = get_optimal_thresholds(dataset_type)

    # ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸Šæ›¸ã
    if custom_threshold is not None:
        standard_threshold = custom_threshold
        logger.info(f"ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤ã‚’ä½¿ç”¨: {custom_threshold}")
    else:
        standard_threshold = thresholds["standard"]

    # åŸºæœ¬ã‚«ãƒãƒ¬ãƒ¼ã‚¸ï¼ˆæ¨™æº–é–¾å€¤ï¼‰
    max_similarities = coverage_matrix.max(axis=1)
    covered_count = sum(1 for s in max_similarities if s >= standard_threshold)
    coverage_rate = covered_count / len(chunks) if chunks else 0

    # æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯ã®ç‰¹å®š
    uncovered_chunks = []
    for i, (chunk, sim) in enumerate(zip(chunks, max_similarities)):
        if sim < standard_threshold:
            uncovered_chunks.append({
                'chunk': chunk,
                'similarity': float(sim),
                'gap': float(standard_threshold - sim)
            })

    # ææ¡ˆ1ã®æ©Ÿèƒ½: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
    logger.info("å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æå®Ÿè¡Œä¸­...")
    multi_threshold_results = multi_threshold_coverage(coverage_matrix, chunks, qa_pairs, thresholds)

    # ææ¡ˆ1ã®æ©Ÿèƒ½: ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æ
    logger.info("ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æå®Ÿè¡Œä¸­...")
    chunk_characteristics = analyze_chunk_characteristics_coverage(
        chunks, coverage_matrix, qa_pairs, standard_threshold
    )

    # çµæœã‚’çµ±åˆ
    results = {
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        "coverage_rate": coverage_rate,
        "covered_chunks": covered_count,
        "total_chunks": len(chunks),
        "uncovered_chunks": uncovered_chunks,
        "max_similarities": max_similarities.tolist(),
        "threshold": standard_threshold,

        # ææ¡ˆ1: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸
        "multi_threshold": multi_threshold_results,

        # ææ¡ˆ1: ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æ
        "chunk_analysis": chunk_characteristics,

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
        "dataset_type": dataset_type,
        "optimal_thresholds": thresholds
    }

    # åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"""
    å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ:
    - Strict  (é–¾å€¤{thresholds['strict']:.2f}): {multi_threshold_results['strict']['coverage_rate']:.1%}
    - Standard(é–¾å€¤{thresholds['standard']:.2f}): {multi_threshold_results['standard']['coverage_rate']:.1%}
    - Lenient (é–¾å€¤{thresholds['lenient']:.2f}): {multi_threshold_results['lenient']['coverage_rate']:.1%}

    ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸:
    é•·ã•åˆ¥:
    - Short ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('short', {}).get('coverage_rate', 0):.1%}
    - Medium ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('medium', {}).get('coverage_rate', 0):.1%}
    - Long ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('long', {}).get('coverage_rate', 0):.1%}

    ä½ç½®åˆ¥:
    - Beginning (å‰åŠ): {chunk_characteristics['by_position'].get('beginning', {}).get('coverage_rate', 0):.1%}
    - Middle (ä¸­ç›¤): {chunk_characteristics['by_position'].get('middle', {}).get('coverage_rate', 0):.1%}
    - End (å¾ŒåŠ): {chunk_characteristics['by_position'].get('end', {}).get('coverage_rate', 0):.1%}
    """)

    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
    if chunk_characteristics['summary']['insights']:
        logger.info("\nğŸ“Š åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
        for insight in chunk_characteristics['summary']['insights']:
            logger.info(f"  â€¢ {insight}")

    return results


# ==========================================
# çµæœä¿å­˜
# ==========================================

def save_results(
    qa_pairs: List[Dict],
    coverage_results: Dict,
    dataset_type: str,
    output_dir: str = "qa_output/a02"
) -> Dict[str, str]:
    """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    Args:
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        coverage_results: ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    Returns:
        ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆJSONï¼‰
    qa_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.json"
    with open(qa_file, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆCSV - å…¨ã‚«ãƒ©ãƒ ï¼‰
    qa_csv_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.csv"
    qa_df = pd.DataFrame(qa_pairs)
    qa_df.to_csv(qa_csv_file, index=False, encoding='utf-8')

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆCSV - question/answerã®ã¿ã®çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
    qa_simple_file = Path("qa_output") / f"a02_qa_pairs_{dataset_type}.csv"
    qa_simple_file.parent.mkdir(parents=True, exist_ok=True)
    if 'question' in qa_df.columns and 'answer' in qa_df.columns:
        qa_simple_df = qa_df[['question', 'answer']]
        qa_simple_df.to_csv(qa_simple_file, index=False, encoding='utf-8')
        logger.info(f"çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆCSVä¿å­˜: {qa_simple_file} ({len(qa_simple_df)}ä»¶)")

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœã‚’ä¿å­˜
    coverage_file = output_path / f"coverage_{dataset_type}_{timestamp}.json"
    # uncovered_chunksã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯¾ç­–
    coverage_save = coverage_results.copy()
    coverage_save['uncovered_chunks'] = [
        {
            'chunk_id': uc['chunk'].get('id', ''),
            'similarity': uc['similarity'],
            'gap': uc['gap'],
            'text_preview': uc['chunk']['text'][:200] + '...'
        }
        for uc in coverage_save.get('uncovered_chunks', [])
    ]

    with open(coverage_file, 'w', encoding='utf-8') as f:
        json.dump(coverage_save, f, ensure_ascii=False, indent=2)

    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ä¿å­˜
    dataset_name = DATASET_CONFIGS.get(dataset_type, {}).get("name", dataset_type)
    summary = {
        "dataset_type": dataset_type,
        "dataset_name": dataset_name,
        "generated_at": timestamp,
        "total_qa_pairs": len(qa_pairs),
        "coverage_rate": coverage_results['coverage_rate'],
        "covered_chunks": coverage_results['covered_chunks'],
        "total_chunks": coverage_results['total_chunks'],
        "files": {
            "qa_json": str(qa_file),
            "qa_csv": str(qa_csv_file),
            "coverage": str(coverage_file)
        }
    }

    summary_file = output_path / f"summary_{dataset_type}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return {
        "qa_json": str(qa_file),
        "qa_csv": str(qa_csv_file),
        "coverage": str(coverage_file),
        "summary": str(summary_file)
    }


# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="preprocessedãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆ"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(DATASET_CONFIGS.keys()),
        default=None,
        help="å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ--input-fileã¨ã¯æ’ä»–ï¼‰"
    )
    parser.add_argument(
        "--input-file",
        type=str,
        default=None,
        help="ãƒ­ãƒ¼ã‚«ãƒ«Q/A CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆquestion, answerã‚«ãƒ©ãƒ å¿…é ˆï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gemini-2.0-flash",
        help="ä½¿ç”¨ã™ã‚‹Geminiãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="qa_output/a02",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"
    )
    parser.add_argument(
        "--analyze-coverage",
        action="store_true",
        help="ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æã‚’å®Ÿè¡Œ"
    )
    parser.add_argument(
        "--batch-chunks",
        type=int,
        default=3,
        choices=[1, 2, 3, 4, 5],
        help="1å›ã®APIã§å‡¦ç†ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰"
    )
    parser.add_argument(
        "--merge-chunks",
        action="store_true",
        default=True,
        help="å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã‚’çµ±åˆã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰"
    )
    parser.add_argument(
        "--no-merge-chunks",
        dest="merge_chunks",
        action="store_false",
        help="ãƒãƒ£ãƒ³ã‚¯çµ±åˆã‚’ç„¡åŠ¹åŒ–"
    )
    parser.add_argument(
        "--min-tokens",
        type=int,
        default=150,
        help="çµ±åˆå¯¾è±¡ã®æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 150ï¼‰"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=400,
        help="çµ±åˆå¾Œã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 400ï¼‰"
    )
    parser.add_argument(
        "--use-celery",
        action="store_true",
        help="Celeryã«ã‚ˆã‚‹éåŒæœŸä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨"
    )
    parser.add_argument(
        "--celery-workers",
        type=int,
        default=8,
        help="Celeryãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8, Gemini APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰"
    )
    parser.add_argument(
        "--coverage-threshold",
        type=float,
        default=None,
        help="ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ¤å®šã®é¡ä¼¼åº¦é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©å€¤ï¼‰"
    )

    args = parser.parse_args()

    # APIã‚­ãƒ¼ç¢ºèªï¼ˆGemini APIï¼‰
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key or api_key == "your-google-api-key-here":
        logger.error("GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)

    # --dataset ã¨ --input-file ã®æ’ä»–ãƒã‚§ãƒƒã‚¯
    if args.dataset and args.input_file:
        logger.error("--dataset ã¨ --input-file ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        sys.exit(1)

    if not args.dataset and not args.input_file:
        logger.error("--dataset ã¾ãŸã¯ --input-file ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        sys.exit(1)

    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆã¯dataset_typeã¨configã‚’å‹•çš„ã«ç”Ÿæˆ
    if args.input_file:
        dataset_type = "custom_upload"
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ™ãƒ¼ã‚¹åã‚’å–å¾—
        file_basename = Path(args.input_file).stem

        # è¨€èªã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼šãƒ•ã‚¡ã‚¤ãƒ«åã‚„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¤å®šï¼‰
        # TODO: ã‚ˆã‚Šç²¾å¯†ãªè¨€èªåˆ¤å®šãŒå¿…è¦ãªå ´åˆã¯å¾Œã§å®Ÿè£…
        lang = "ja"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ—¥æœ¬èª

        config = {
            "name": f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« ({file_basename})",
            "text_column": "Combined_Text",
            "title_column": None,
            "lang": lang,
            "chunk_size": 300,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            "qa_per_chunk": 3,
        }

        dataset_name = config['name']
    else:
        dataset_type = args.dataset
        config = DATASET_CONFIGS[dataset_type]
        dataset_name = config['name']

    logger.info(f"""
    =====================================
    Q/Aãƒšã‚¢ç”Ÿæˆé–‹å§‹
    =====================================
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}
    ãƒ¢ãƒ‡ãƒ«: {args.model}
    å‡ºåŠ›å…ˆ: {args.output}
    æœ€å¤§æ–‡æ›¸æ•°: {args.max_docs if args.max_docs else 'åˆ¶é™ãªã—'}
    ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ: {'å®Ÿè¡Œ' if args.analyze_coverage else 'ã‚¹ã‚­ãƒƒãƒ—'}
    """)

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("\n[1/4] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")

        if args.input_file:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            df = load_uploaded_file(args.input_file)

            # max_docsåˆ¶é™ã‚’é©ç”¨
            if args.max_docs and len(df) > args.max_docs:
                df = df.head(args.max_docs)
                logger.info(f"  ğŸ“Š æœ€å¤§æ–‡æ›¸æ•°åˆ¶é™: {len(df)} ä»¶ã«åˆ¶é™")
        else:
            # æ—¢å­˜ã®preprocessedãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = load_preprocessed_data(dataset_type)

        # 1.5 Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®äº‹å‰ç¢ºèªï¼ˆFail Fastï¼‰
        if args.use_celery:
            logger.info("Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
            if not check_celery_workers(args.celery_workers):
                logger.error("Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
                sys.exit(1)
            logger.info(f"âœ“ Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ç¢ºèªOKï¼ˆ{args.celery_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰")

        # 2. ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        logger.info("\n[2/4] ãƒãƒ£ãƒ³ã‚¯ä½œæˆ...")
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€max_docsã¯èª­ã¿è¾¼ã¿æ™‚ã«é©ç”¨æ¸ˆã¿
        max_docs_for_chunks = None if args.input_file else args.max_docs
        chunks = create_document_chunks(df, dataset_type, max_docs_for_chunks, config=config)

        if not chunks:
            logger.error("ãƒãƒ£ãƒ³ã‚¯ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(1)

        # 3. Q/Aãƒšã‚¢ç”Ÿæˆ
        logger.info("\n[3/4] Q/Aãƒšã‚¢ç”Ÿæˆ...")

        if args.use_celery:
            logger.info(f"Celeryä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°={args.celery_workers}")
            logger.info(f"ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒãƒƒãƒã‚µã‚¤ã‚º={args.batch_chunks}, ãƒãƒ£ãƒ³ã‚¯çµ±åˆ={'æœ‰åŠ¹' if args.merge_chunks else 'ç„¡åŠ¹'}")

            # Celeryã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆGeminiå¯¾å¿œã®çµ±åˆç‰ˆã‚’ä½¿ç”¨ï¼‰
            from celery_tasks import submit_unified_qa_generation, collect_results

            # ãƒãƒ£ãƒ³ã‚¯ã®å‰å‡¦ç†ï¼ˆconfigã¯æ—¢ã«å®šç¾©æ¸ˆã¿ï¼‰
            if args.merge_chunks:
                from a02_make_qa_para import merge_small_chunks
                processed_chunks = merge_small_chunks(chunks, args.min_tokens, args.max_tokens)
            else:
                processed_chunks = chunks

            # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯æŠ•å…¥ï¼ˆGemini APIã‚’ä½¿ç”¨ï¼‰
            tasks = submit_unified_qa_generation(
                processed_chunks, config, args.model, provider="gemini"
            )

            # çµæœåé›†ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ã‚¿ã‚¹ã‚¯æ•° Ã— 10ç§’ã€æœ€ä½600ç§’ã€æœ€å¤§1800ç§’ï¼‰
            # å¤§é‡ã‚¿ã‚¹ã‚¯ã®å ´åˆã§ã‚‚30åˆ†ä»¥å†…ã«åé›†å®Œäº†ã‚’æƒ³å®š
            timeout_seconds = min(max(len(tasks) * 10, 600), 1800)
            logger.info(f"çµæœåé›†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout_seconds}ç§’ï¼ˆ{len(tasks)}ã‚¿ã‚¹ã‚¯ï¼‰")
            qa_pairs = collect_results(tasks, timeout=timeout_seconds)
        else:
            logger.info("é€šå¸¸å‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
            logger.info(f"ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒãƒƒãƒã‚µã‚¤ã‚º={args.batch_chunks}, ãƒãƒ£ãƒ³ã‚¯çµ±åˆ={'æœ‰åŠ¹' if args.merge_chunks else 'ç„¡åŠ¹'}")
            qa_pairs = generate_qa_for_dataset(
                chunks,
                dataset_type,
                args.model,
                chunk_batch_size=args.batch_chunks,
                merge_chunks=args.merge_chunks,
                min_tokens=args.min_tokens,
                max_tokens=args.max_tokens,
                config=config
            )

        if not qa_pairs:
            logger.warning("Q/Aãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        # 4. ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        coverage_results = {}
        if args.analyze_coverage and qa_pairs:
            logger.info("\n[4/4] ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆEmbeddingç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰...")
            coverage_results = analyze_coverage(
                chunks, qa_pairs, dataset_type,
                custom_threshold=args.coverage_threshold
            )

            logger.info(f"""
            ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ:
            - ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡: {coverage_results['coverage_rate']:.1%}
            - ã‚«ãƒãƒ¼æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯: {coverage_results['covered_chunks']}/{coverage_results['total_chunks']}
            - æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯: {len(coverage_results['uncovered_chunks'])}
            """)
        else:
            logger.info("\n[4/4] ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
            coverage_results = {
                "coverage_rate": 0,
                "covered_chunks": 0,
                "total_chunks": len(chunks),
                "uncovered_chunks": []
            }

        # 5. çµæœä¿å­˜
        logger.info("\nçµæœã‚’ä¿å­˜ä¸­...")
        saved_files = save_results(qa_pairs, coverage_results, dataset_type, args.output)

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        logger.info(f"""
        =====================================
        å‡¦ç†å®Œäº†
        =====================================
        ç”ŸæˆQ/Aãƒšã‚¢æ•°: {len(qa_pairs)}
        ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
        - Q/A (JSON): {saved_files['qa_json']}
        - Q/A (CSV): {saved_files['qa_csv']}
        - ã‚«ãƒãƒ¬ãƒ¼ã‚¸: {saved_files['coverage']}
        - ã‚µãƒãƒªãƒ¼: {saved_files['summary']}
        """)

        # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
        if qa_pairs:
            question_types = {}
            for qa in qa_pairs:
                qt = qa.get('question_type', 'unknown')
                question_types[qt] = question_types.get(qt, 0) + 1

            print("\nè³ªå•ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
            for qt, count in sorted(question_types.items()):
                print(f"  {qt}: {count}ä»¶")

        # Celeryã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆï¼‰
        if args.use_celery:
            try:
                # Celeryã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                from celery_tasks import app as celery_app
                # æ¥ç¶šã‚’é–‰ã˜ã‚‹
                celery_app.close()
                logger.debug("Celeryæ¥ç¶šã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
            except Exception as cleanup_error:
                logger.debug(f"Celeryã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ™‚ã®è­¦å‘Š: {cleanup_error}")

        # æ­£å¸¸çµ‚äº†
        sys.exit(0)

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()